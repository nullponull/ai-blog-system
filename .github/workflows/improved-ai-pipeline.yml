name: Advanced AI Article Pipeline with Transformer Analysis

on:
  schedule:
    - cron: '15 */8 * * *'  # 8ÊôÇÈñì„Åî„Å®
  workflow_dispatch:

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Python dependencies
        run: |
          pip install transformers torch sentence-transformers scikit-learn numpy
          pip install requests beautifulsoup4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Node dependencies
        run: |
          npm install
          npm install -g @google/gemini-cli
          mkdir -p _temp

      - name: Advanced Topic Generation with Semantic Analysis
        id: generate_topics
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          export GEMINI_API_KEY="$GEMINI_API_KEY"
          DATE_STR=$(date '+%YÂπ¥%mÊúà%dÊó•')
          
          # Create Python script for semantic analysis
          cat > _temp/semantic_setup.py << 'PYTHON_EOF'
import os
from transformers import AutoTokenizer, AutoModel
import torch
import json
from datetime import datetime, timedelta
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import glob

print('üß† Setting up Transformer-based semantic analysis...')

# Load Japanese BERT model for semantic understanding
model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_embedding(text):
    """Get semantic embedding for text"""
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).numpy()

# Load existing articles for semantic comparison
existing_articles = []
for file_path in glob.glob('_posts/*.md'):
    if os.path.getmtime(file_path) > (datetime.now() - timedelta(days=3)).timestamp():
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # Extract title and content
            lines = content.split('\n')
            title = ''
            for line in lines:
                if line.startswith('title:'):
                    title = line.replace('title:', '').strip().strip('"')
                    break
            if title:
                existing_articles.append({
                    'title': title,
                    'file': file_path,
                    'embedding': get_embedding(title)
                })

print(f'üìö Loaded {len(existing_articles)} existing articles for semantic comparison')

# Save existing articles data for later use
with open('_temp/existing_articles.json', 'w', encoding='utf-8') as f:
    # Convert numpy arrays to lists for JSON serialization
    articles_data = []
    for article in existing_articles:
        articles_data.append({
            'title': article['title'],
            'file': article['file'],
            'embedding': article['embedding'].tolist()
        })
    json.dump(articles_data, f, ensure_ascii=False, indent=2)

print('‚úÖ Semantic analysis setup completed')
PYTHON_EOF
          
          # Execute the semantic setup
          python3 _temp/semantic_setup.py
          
          # Generate diverse topics with semantic awareness
          gemini -m "gemini-2.5-flash" --prompt "WebSearch: AIÊ•≠Áïå ÊúÄÊñ∞„Éã„É•„Éº„Çπ 2025„ÄÇ‰ª•‰∏ã„ÅÆÊù°‰ª∂„ÅßÂ§öÊßòÊÄß„ÇíÈáçË¶ñ„Åó„ÅüAIÊ•≠Áïå„ÅÆË©±È°å„Çí15ÂÄãÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

Êù°‰ª∂Ôºö
1. Áï∞„Å™„ÇãÂàÜÈáé„ÉªÊ•≠Áïå„Çí„Ç´„Éê„ÉºÔºàÂåªÁôÇ„ÄÅÈáëËûç„ÄÅË£ΩÈÄ†Ê•≠„ÄÅ„Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É°„É≥„Éà„Å™„Å©Ôºâ
2. Áï∞„Å™„ÇãÊäÄË°ìÈ†òÂüüÔºàLLM„ÄÅÁîªÂÉèÁîüÊàê„ÄÅÈü≥Â£∞Ë™çË≠ò„ÄÅ„É≠„Éú„ÉÜ„Ç£„ÇØ„Çπ„Å™„Å©Ôºâ  
3. Áï∞„Å™„ÇãË¶≥ÁÇπÔºàÊäÄË°ì„ÄÅ„Éì„Ç∏„Éç„Çπ„ÄÅË¶èÂà∂„ÄÅÁ§æ‰ºöÂΩ±Èüø„Å™„Å©Ôºâ
4. ÂÖ∑‰ΩìÁöÑ„Å™‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„Éª‰∫∫Âêç„ÇíÂê´„ÇÅ„Çã
5. ÊúÄÊñ∞„ÅÆÊó•‰ªò„ÉªÊï∞ÂÄ§„Éª„Éá„Éº„Çø„ÇíÂê´„ÇÅ„Çã

ÂΩ¢ÂºèÔºö
1. [ÂàÜÈáé] [‰ºÅÊ•≠/ÊäÄË°ì] - [ÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ]
‰æãÔºö
1. ÂåªÁôÇAI DeepMind - „Çø„É≥„Éë„ÇØË≥™ÊßãÈÄ†‰∫àÊ∏¨AlphaFold3„ÅåÊñ∞Ëñ¨ÈñãÁô∫„Çí30%Âä†ÈÄü
2. ÈáëËûçÊäÄË°ì JPMorgan - AIÊäïË≥á„Ç¢„É´„Ç¥„É™„Ç∫„É†„Åå2025Âπ¥Q1„Åß15%„ÅÆ„É™„Çø„Éº„É≥ÈÅîÊàê
3. Ëá™ÂãïÈÅãËª¢ Tesla - FSD v13„ÅåÈ´òÈÄüÈÅìË∑Ø„Åß„ÅÆ‰∫ãÊïÖÁéá„Çí40%ÂâäÊ∏õ
4. ÁîüÊàêAI Anthropic - Claude 3.5„ÅåÊó•Êú¨Ë™ûÂá¶ÁêÜËÉΩÂäõ„ÅßGPT-4„Çí‰∏äÂõû„ÇãÊÄßËÉΩ
5. ÈáèÂ≠êAI IBM - ÈáèÂ≠êÊ©üÊ¢∞Â≠¶Áøí„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅåÂæìÊù•ÊØî1000ÂÄçÈ´òÈÄüÂåñ

„Åì„ÅÆ„Çà„ÅÜ„Å™ÂÖ∑‰ΩìÊÄß„Å®Â§öÊßòÊÄß„ÇíÊåÅ„Å£„Åü15„ÅÆË©±È°å„Çí„É™„Çπ„Éà„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ" > _temp/diverse-topics.txt
          
          echo "Generated diverse topics:"
          cat _temp/diverse-topics.txt

      - name: Semantic Duplicate Detection & Article Generation
        id: generate_articles
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          export GEMINI_API_KEY="$GEMINI_API_KEY"
          
          # Create Python script for semantic analysis
          cat > _temp/semantic_analysis.py << 'PYTHON_EOF'
import json
import numpy as np
import subprocess
import os
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import torch

print('üî¨ Starting advanced semantic analysis...')

# Load model
model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

def get_embedding(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).numpy()

# Load existing articles
existing_articles = []
if os.path.exists('_temp/existing_articles.json'):
    with open('_temp/existing_articles.json', 'r', encoding='utf-8') as f:
        articles_data = json.load(f)
        for article in articles_data:
            article['embedding'] = np.array(article['embedding'])
            existing_articles.append(article)

# Load generated topics
with open('_temp/diverse-topics.txt', 'r', encoding='utf-8') as f:
    topics_content = f.read()

topics = []
for line in topics_content.split('\n'):
    if line.strip() and line[0].isdigit():
        topic = line.split('.', 1)[1].strip()
        if topic:
            topics.append(topic)

print(f'üìã Processing {len(topics)} topics')

# Semantic similarity analysis
selected_topics = []
SIMILARITY_THRESHOLD = 0.75  # More relaxed threshold
MAX_ARTICLES = 8

for i, topic in enumerate(topics[:12]):  # Check more topics
    if len(selected_topics) >= MAX_ARTICLES:
        break
        
    print(f'\nüîç Analyzing topic {i+1}: {topic[:60]}...')
    topic_embedding = get_embedding(topic)
    
    # Check similarity with existing articles
    max_similarity = 0.0
    most_similar = None
    
    for existing in existing_articles:
        similarity = cosine_similarity(
            topic_embedding.reshape(1, -1), 
            existing['embedding'].reshape(1, -1)
        )[0][0]
        
        if similarity > max_similarity:
            max_similarity = similarity
            most_similar = existing['title']
    
    # Check similarity with already selected topics
    selected_similarity = 0.0
    for selected in selected_topics:
        selected_embedding = get_embedding(selected)
        similarity = cosine_similarity(
            topic_embedding.reshape(1, -1),
            selected_embedding.reshape(1, -1)
        )[0][0]
        selected_similarity = max(selected_similarity, similarity)
    
    print(f'üìä Similarity scores: Existing={max_similarity:.3f}, Selected={selected_similarity:.3f}')
    
    # More intelligent selection logic
    if max_similarity < SIMILARITY_THRESHOLD and selected_similarity < SIMILARITY_THRESHOLD:
        selected_topics.append(topic)
        print(f'‚úÖ SELECTED: Low similarity, adding to queue')
    elif max_similarity < SIMILARITY_THRESHOLD + 0.1 and len(selected_topics) < 4:
        # Force some articles even with moderate similarity
        selected_topics.append(topic)
        print(f'üîÑ FORCED: Ensuring minimum content generation')
    else:
        print(f'‚ùå SKIPPED: Too similar (existing: {most_similar})')

print(f'\nüìù Final selection: {len(selected_topics)} topics')
for i, topic in enumerate(selected_topics, 1):
    print(f'{i}. {topic}')

# Save selected topics
with open('_temp/selected-topics.json', 'w', encoding='utf-8') as f:
    json.dump(selected_topics, f, ensure_ascii=False, indent=2)
PYTHON_EOF
          
          # Execute semantic analysis
          python3 _temp/semantic_analysis.py
          
          # Create Python script for article generation
          cat > _temp/generate_articles.py << 'PYTHON_EOF'
import json
import subprocess
import os
from datetime import datetime

# Load selected topics
with open('_temp/selected-topics.json', 'r', encoding='utf-8') as f:
    selected_topics = json.load(f)

print(f'üöÄ Generating {len(selected_topics)} articles...')

for i, topic in enumerate(selected_topics, 1):
    print(f'\nüìù Generating article {i}: {topic[:60]}...')
    
    # Determine category and create specialized prompt
    category = 'ÊúÄÊñ∞ÂãïÂêë'  # Default
    if any(word in topic.lower() for word in ['Á†îÁ©∂', 'Ë´ñÊñá', 'paper', 'Â≠¶‰ºö']):
        category = 'Á†îÁ©∂Ë´ñÊñá'
    elif any(word in topic.lower() for word in ['ÊäÄË°ì', '„Ç¢„É´„Ç¥„É™„Ç∫„É†', '‰ªïÁµÑ„Åø']):
        category = 'ÊäÄË°ìËß£Ë™¨'
    elif any(word in topic.lower() for word in ['Â∞éÂÖ•', '‰∫ã‰æã', 'Ê¥ªÁî®']):
        category = 'ÂÆüË£Ö‰∫ã‰æã'
    elif any(word in topic.lower() for word in ['Â∏ÇÂ†¥', 'ÂàÜÊûê', '‰∫àÊ∏¨']):
        category = 'Ê•≠ÁïåÂàÜÊûê'
    
    prompt = f'''WebSearch: {datetime.now().strftime('%YÂπ¥%mÊúà%dÊó•')} {topic}„ÄÇ
„Äå{topic}„Äç„Å´„Å§„ÅÑ„Å¶„ÄÅWebÊ§úÁ¥¢„ÅßÊúÄÊñ∞ÊÉÖÂ†±„ÇíË™øÊüª„Åó„ÄÅALLFORCES AIÊÉÖÂ†±„É°„Éá„Ç£„Ç¢Âêë„Åë„ÅÆÂ∞ÇÈñÄË®ò‰∫ã„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

Ë¶Å‰ª∂Ôºö
- ÂÆüÈöõ„ÅÆÊúÄÊñ∞ÊÉÖÂ†±„Å´Âü∫„Å•„ÅèÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ
- ‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„Éª‰∫∫Âêç„ÉªÊï∞ÂÄ§„Éá„Éº„Çø„ÇíÊ≠£Á¢∫„Å´Ë®òËºâ
- ÊäÄË°ìËÄÖ„ÉªÊäïË≥áÂÆ∂Âêë„Åë„ÅÆË©≥Á¥∞„Å™ÂàÜÊûê
- 3000-4000ÊñáÂ≠óÁ®ãÂ∫¶
- MarkdownÂΩ¢Âºè„ÅßÂá∫Âäõ

ÊßãÊàêÔºö
# {topic}
## Ê¶ÇË¶Å„Å®ËÉåÊôØ
## Ë©≥Á¥∞„Å™ÊäÄË°ì„Éª„Éì„Ç∏„Éç„ÇπÂÜÖÂÆπ
## Â∏ÇÂ†¥„ÉªÁ´∂Âêà„Å∏„ÅÆÂΩ±Èüø
## ‰ªäÂæå„ÅÆÂ±ïÊúõ

Â∞ÇÈñÄÊÄß„Å®‰ø°È†ºÊÄß„ÇíÈáçË¶ñ„Åó„ÅüË®ò‰∫ã„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ'''
    
    # Generate article using Gemini
    result = subprocess.run([
        'gemini', '-m', 'gemini-2.5-flash', '--prompt', prompt
    ], capture_output=True, text=True, env=dict(os.environ))
    
    if result.returncode == 0 and result.stdout.strip():
        # Save article
        filename = f'_temp/article-{i}.md'
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(result.stdout)
        
        print(f'‚úÖ Generated: {filename}')
        
        # Create Jekyll post
        title = topic[:100]  # Truncate long titles
        slug = topic.lower().replace(' ', '-')[:50]
        post_filename = f'{datetime.now().strftime("%Y-%m-%d")}-{i}-{slug}.md'
        
        # Category-specific tags and excerpts
        tags_map = {
            'ÊúÄÊñ∞ÂãïÂêë': '["AI", "ÊúÄÊñ∞„Éã„É•„Éº„Çπ", "Ê•≠ÁïåÂãïÂêë", "‰ºÅÊ•≠Áô∫Ë°®"]',
            'ÊäÄË°ìËß£Ë™¨': '["AI", "ÊäÄË°ìËß£Ë™¨", "„Ç¢„É´„Ç¥„É™„Ç∫„É†", "ÂÆüË£Ö"]',
            'ÂÆüË£Ö‰∫ã‰æã': '["AI", "Â∞éÂÖ•‰∫ã‰æã", "ÂÆüË£Ö", "ROI"]',
            'Ê•≠ÁïåÂàÜÊûê': '["AI", "Â∏ÇÂ†¥ÂàÜÊûê", "„Éà„É¨„É≥„Éâ", "ÊäïË≥á"]',
            'Á†îÁ©∂Ë´ñÊñá': '["AI", "Á†îÁ©∂Ë´ñÊñá", "Â≠¶Ë°ì", "ÊúÄÊñ∞Á†îÁ©∂"]'
        }
        
        excerpt_map = {
            'ÊúÄÊñ∞ÂãïÂêë': 'AIÊ•≠Áïå„ÅÆÊúÄÊñ∞ÂãïÂêë„Å´„Å§„ÅÑ„Å¶„ÄÅ‰ºÅÊ•≠Áô∫Ë°®„ÇÑÊ•≠Áïå„Éã„É•„Éº„Çπ„ÇíÂü∫„Å´Ë©≥„Åó„ÅèËß£Ë™¨„Åó„Åæ„Åô„ÄÇ',
            'ÊäÄË°ìËß£Ë™¨': 'AIÊäÄË°ì„ÅÆ‰ªïÁµÑ„Åø„Å®„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´„Å§„ÅÑ„Å¶„ÄÅÊäÄË°ìÁöÑ„Å™Ë¶≥ÁÇπ„Åã„ÇâË©≥„Åó„ÅèËß£Ë™¨„Åó„Åæ„Åô„ÄÇ',
            'ÂÆüË£Ö‰∫ã‰æã': '‰ºÅÊ•≠„ÅÆAIÂ∞éÂÖ•‰∫ã‰æã„Å®ÂÆüË£ÖÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶„ÄÅÂÖ∑‰ΩìÁöÑ„Å™ÊàêÊûú„ÇíÂê´„ÇÅ„Å¶Ëß£Ë™¨„Åó„Åæ„Åô„ÄÇ',
            'Ê•≠ÁïåÂàÜÊûê': 'AIÊ•≠Áïå„ÅÆÂ∏ÇÂ†¥ÂãïÂêë„Å®Â∞ÜÊù•Â±ïÊúõ„Å´„Å§„ÅÑ„Å¶„ÄÅ„Éá„Éº„Çø„Å´Âü∫„Å•„ÅÑ„Å¶ÂàÜÊûê„Åó„Åæ„Åô„ÄÇ',
            'Á†îÁ©∂Ë´ñÊñá': 'AIÂàÜÈáé„ÅÆÊúÄÊñ∞Á†îÁ©∂Ë´ñÊñá„Å´„Å§„ÅÑ„Å¶„ÄÅÊäÄË°ìÁöÑ„Å™ÂÜÖÂÆπ„ÇíÂàÜ„Åã„Çä„ÇÑ„Åô„ÅèËß£Ë™¨„Åó„Åæ„Åô„ÄÇ'
        }
        
        # Create final post
        with open(f'_posts/{post_filename}', 'w', encoding='utf-8') as f:
            f.write(f'''---
layout: post
title: "{title}"
date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S %z')}
categories: ["{category}"]
tags: {tags_map.get(category, '["AI", "Ê©üÊ¢∞Â≠¶Áøí", "ÊäÄË°ìËß£Ë™¨"]')}
author: "AIË®ò‰∫ãÁîüÊàê„Ç∑„Çπ„ÉÜ„É†"
excerpt: "{excerpt_map.get(category, 'AI„Å´Èñ¢„Åô„ÇãÊúÄÊñ∞ÊÉÖÂ†±„ÇíË©≥„Åó„ÅèËß£Ë™¨„Åó„Åæ„Åô„ÄÇ')}"
reading_time: 8
---

''')
            # Add article content (remove markdown code blocks if present)
            content = result.stdout
            if content.startswith('```markdown'):
                content = '\n'.join(content.split('\n')[1:])
            if content.endswith('```'):
                content = '\n'.join(content.split('\n')[:-1])
            f.write(content)
        
        print(f'üìÑ Created post: {post_filename}')
    else:
        print(f'‚ùå Failed to generate article for: {topic}')

print('\nüéâ Article generation completed!')
PYTHON_EOF
          
          # Execute article generation
          python3 _temp/generate_articles.py

      - name: Quality Check and Enhancement
        run: |
          echo "üîç Performing quality checks on generated articles..."
          
          # Count generated articles
          ARTICLE_COUNT=$(ls _posts/$(date +%Y-%m-%d)-*.md 2>/dev/null | wc -l)
          echo "üìä Generated $ARTICLE_COUNT articles today"
          
          # Basic quality checks
          for file in _posts/$(date +%Y-%m-%d)-*.md; do
            if [ -f "$file" ]; then
              WORD_COUNT=$(wc -w < "$file")
              echo "üìù $(basename "$file"): $WORD_COUNT words"
              
              # Ensure minimum quality
              if [ "$WORD_COUNT" -lt 500 ]; then
                echo "‚ö†Ô∏è  Warning: Article too short, removing: $file"
                rm "$file"
              fi
            fi
          done

      - name: Setup Ruby and Jekyll
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.1'
          bundler-cache: true

      - name: Build and Deploy
        run: |
          bundle install
          bundle exec jekyll build
          
      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './_site'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Commit generated articles
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          if [ -n "$(git status --porcelain _posts/)" ]; then
            ARTICLE_COUNT=$(ls _posts/$(date +%Y-%m-%d)-*.md 2>/dev/null | wc -l)
            git add _posts/
            git commit -m "ü§ñ Add $ARTICLE_COUNT AI articles with semantic analysis - $(date +%Y-%m-%d)"
            git push
            echo "‚úÖ Committed $ARTICLE_COUNT new articles"
          else
            echo "‚ÑπÔ∏è  No new articles to commit"
          fi