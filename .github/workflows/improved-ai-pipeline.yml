name: AI Article Generation & Publishing Pipeline

on:
  schedule:
    - cron: '15 */8 * * *'  # 8æ™‚é–“ã”ã¨
  workflow_dispatch:

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          npm install -g @google/gemini-cli textlint textlint-rule-preset-japanese textlint-rule-preset-ja-technical-writing @textlint-ja/textlint-rule-preset-ai-writing
          pip install sentence-transformers scikit-learn numpy requests pillow
          mkdir -p _temp assets/images/posts

      - name: Generate topics and articles
        shell: bash
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY2 }}
        run: |
          export GEMINI_API_KEY="$GEMINI_API_KEY"
          
          echo "ğŸ¯ Generating AI topics..."
          echo "Debug: GEMINI_API_KEY is $([ -n "$GEMINI_API_KEY" ] && echo "set (length: ${#GEMINI_API_KEY})" || echo "NOT SET")"
          
          # GitHub Actionsç’°å¢ƒã§ã®API keyèªè¨¼ç¢ºèª
          if [ -z "$GEMINI_API_KEY" ]; then
            echo "âŒ GEMINI_API_KEY is not set. Please check GitHub Secrets configuration."
            exit 1
          fi
          
          # Gemini CLI with API key authentication
          echo "Testing Gemini API connection..."
          if ! gemini --version; then
            echo "âŒ Gemini CLI not found or not working"
            exit 1
          fi
          
          # Optimized API call: Generate 8 diverse topics in one call (API optimization)
          echo "ğŸ¯ Optimized topic generation (single API call)..."
          OPTIMIZED_PROMPT="WebSearch: AIæ¥­ç•Œ æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‚

**ãƒŸãƒƒã‚·ãƒ§ãƒ³:** AIæ¥­ç•Œå°‚é–€è¨˜è€…ã¨ã—ã¦ã€Webæ¤œç´¢ã§æœ€æ–°æƒ…å ±ã‚’èª¿æŸ»ã—ã€å¤šæ§˜ãªåˆ†é‡ã‹ã‚‰8å€‹ã®è³ªã®é«˜ã„è©±é¡Œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

**å¤šæ§˜æ€§è¦ä»¶ï¼ˆå„åˆ†é‡2å€‹ãšã¤ï¼‰:**
1. **ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹å‹•å‘:** æ–°è£½å“ç™ºè¡¨ã€æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”ã€M&Aã€IPOã€æˆ¦ç•¥ç™ºè¡¨
2. **æŠ€è¡“ãƒ»ç ”ç©¶é–‹ç™º:** æ–°æŠ€è¡“ã€ç ”ç©¶æˆæœã€è«–æ–‡ç™ºè¡¨ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é©æ–°
3. **æ”¿ç­–ãƒ»è¦åˆ¶:** æ³•è¦åˆ¶ã€æ”¿åºœæ”¿ç­–ã€å›½éš›å”åŠ›ã€ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ç­–å®š
4. **ç¤¾ä¼šå®Ÿè£…ãƒ»äº‹ä¾‹:** ä¼æ¥­å°å…¥äº‹ä¾‹ã€æ¥­ç•Œå¤‰é©ã€å®Ÿç”¨åŒ–ãƒ»å•†ç”¨åŒ–äº‹ä¾‹

**å‡ºåŠ›å½¢å¼ï¼ˆå³å¯†ã«å¾“ã£ã¦ãã ã•ã„ï¼‰:**
1. [ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹] å…·ä½“çš„ãªè©±é¡Œï¼šä¼æ¥­åãƒ»è£½å“åãƒ»é‡‘é¡ç­‰ã‚’å«ã‚€
2. [ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹] å…·ä½“çš„ãªè©±é¡Œï¼šä¼æ¥­åãƒ»è£½å“åãƒ»é‡‘é¡ç­‰ã‚’å«ã‚€  
3. [æŠ€è¡“ãƒ»ç ”ç©¶] å…·ä½“çš„ãªè©±é¡Œï¼šç ”ç©¶è€…åãƒ»å¤§å­¦åãƒ»æŠ€è¡“åã‚’å«ã‚€
4. [æŠ€è¡“ãƒ»ç ”ç©¶] å…·ä½“çš„ãªè©±é¡Œï¼šç ”ç©¶è€…åãƒ»å¤§å­¦åãƒ»æŠ€è¡“åã‚’å«ã‚€
5. [æ”¿ç­–ãƒ»è¦åˆ¶] å…·ä½“çš„ãªè©±é¡Œï¼šå›½åãƒ»æ©Ÿé–¢åãƒ»æ”¿ç­–åã‚’å«ã‚€
6. [æ”¿ç­–ãƒ»è¦åˆ¶] å…·ä½“çš„ãªè©±é¡Œï¼šå›½åãƒ»æ©Ÿé–¢åãƒ»æ”¿ç­–åã‚’å«ã‚€
7. [ç¤¾ä¼šå®Ÿè£…] å…·ä½“çš„ãªè©±é¡Œï¼šä¼æ¥­åãƒ»äº‹ä¾‹ãƒ»å½±éŸ¿ã‚’å«ã‚€
8. [ç¤¾ä¼šå®Ÿè£…] å…·ä½“çš„ãªè©±é¡Œï¼šä¼æ¥­åãƒ»äº‹ä¾‹ãƒ»å½±éŸ¿ã‚’å«ã‚€

**å“è³ªè¦ä»¶:**
- Webæ¤œç´¢ã«ã‚ˆã‚‹æœ€æ–°ãƒ»æ­£ç¢ºãªæƒ…å ±ä½¿ç”¨
- å…·ä½“çš„ãªå›ºæœ‰åè©ï¼ˆä¼æ¥­åãƒ»äººåãƒ»è£½å“åãƒ»æ•°å€¤ï¼‰å¿…é ˆ
- å„è©±é¡Œã¯æ™‚äº‹æ€§ã¨å°‚é–€æ€§ã‚’ä¸¡ç«‹"
          
          if gemini -m "gemini-2.5-flash" --prompt "$OPTIMIZED_PROMPT" > _temp/topics.txt 2>_temp/gemini_error.log; then
            echo "âœ… Optimized topic generation successful (1 API call for 8 topics)"
          else
            echo "âŒ Optimized call failed, trying fallback..."
            cat _temp/gemini_error.log || echo "No error log available"
            
            FALLBACK_PROMPT="AIæ¥­ç•Œã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰å¤šæ§˜ãª8å€‹ã®è©±é¡Œã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚

**åˆ†é‡åˆ¥è¦ä»¶ï¼ˆå„2å€‹ï¼‰:**
- ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹: æ–°è£½å“ã€æŠ•è³‡ã€M&Aã€æˆ¦ç•¥ç™ºè¡¨
- æŠ€è¡“ãƒ»ç ”ç©¶: æ–°æŠ€è¡“ã€ç ”ç©¶æˆæœã€è«–æ–‡ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- æ”¿ç­–ãƒ»è¦åˆ¶: æ³•è¦åˆ¶ã€æ”¿åºœæ”¿ç­–ã€å›½éš›å”åŠ›
- ç¤¾ä¼šå®Ÿè£…: å°å…¥äº‹ä¾‹ã€æ¥­ç•Œå¤‰é©ã€å®Ÿç”¨åŒ–

**å‡ºåŠ›:** 1-8ã®ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§ã€ä¼æ¥­åãƒ»è£½å“åãƒ»äººåã‚’å«ã‚€å…·ä½“çš„ãªè©±é¡Œ"
            
            if gemini -m "gemini-2.5-flash" --prompt "$FALLBACK_PROMPT" > _temp/topics.txt 2>_temp/fallback_error.log; then
              echo "âœ… Fallback successful (1 API call)"
            else
              echo "âŒ Topic generation completely failed"
              exit 1
            fi
          fi
          
          echo "Generated topics:"
          head -15 _temp/topics.txt
          
          # Extract and filter duplicate topics
          ALL_GENERATED_TOPICS=$(grep -E "^[0-9]+\." _temp/topics.txt)
          echo "ğŸ“ Initial topics generated:"
          echo "$ALL_GENERATED_TOPICS"
          
          if [ -z "$ALL_GENERATED_TOPICS" ]; then
            echo "âŒ No topics generated, exiting..."
            exit 1
          fi
          
          # Get existing titles for duplicate check (last 5 days)
          if ls _posts/*.md 1> /dev/null 2>&1; then
            find _posts -name "*.md" -mtime -5 -exec grep -h "^title:" {} \; 2>/dev/null | sed 's/^title: *["]*\|["]*$//g' > _temp/existing_titles.txt
          else
            touch _temp/existing_titles.txt
          fi
          
          echo "ğŸ” Filtering out duplicate topics..."
          TOPICS=""
          TOPIC_COUNT=0
          RETRY_COUNT=0
          MAX_RETRIES=3
          
          # Filter duplicates and get unique topics (reduced to 5 to stay within API limits)
          while IFS= read -r topic_line && [ $TOPIC_COUNT -lt 5 ]; do
            if [ -z "$topic_line" ]; then
              continue
            fi
            
            topic_title=$(printf '%s\n' "$topic_line" | sed 's/^[0-9]*\. *//')
            echo "ğŸ” Checking: $topic_title"
            
            # Simple keyword-based duplicate check
            is_duplicate=false
            if [ -s _temp/existing_titles.txt ]; then
              # Extract key words from topic (first 3 significant words)
              topic_keywords=$(echo "$topic_title" | sed 's/[()ï¼ˆï¼‰].*//g' | head -c 30)
              while IFS= read -r existing_title; do
                if [ -n "$existing_title" ] && echo "$existing_title" | grep -qi "$(echo "$topic_keywords" | cut -d' ' -f1-2)"; then
                  echo "â­ï¸ Duplicate detected: $topic_title (similar to existing: $existing_title)"
                  is_duplicate=true
                  break
                fi
              done < _temp/existing_titles.txt
            fi
            
            if [ "$is_duplicate" = false ]; then
              if [ -z "$TOPICS" ]; then
                TOPICS="$topic_line"
              else
                TOPICS="$TOPICS"$'\n'"$topic_line"
              fi
              TOPIC_COUNT=$((TOPIC_COUNT + 1))
              echo "âœ… Added unique topic: $topic_title"
            fi
          done <<< "$ALL_GENERATED_TOPICS"
          
          # Simple selection from the 8 generated topics (no additional API calls)
          echo "ğŸ“ Selecting best 5 topics from the 8 generated..."
          
          # Final check: if we still don't have enough topics after all retries
          if [ $TOPIC_COUNT -lt 5 ]; then
            echo "âš ï¸ Warning: Only $TOPIC_COUNT unique topics found after $MAX_RETRIES retries"
            echo "âš ï¸ This might indicate topic exhaustion or API issues"
            echo "âš ï¸ Proceeding with available topics to avoid infinite processing"
          elif [ $TOPIC_COUNT -lt 10 ]; then
            echo "â„¹ï¸ Note: Found $TOPIC_COUNT topics (less than target 10)"
            echo "â„¹ï¸ Proceeding with available unique topics"
          fi
          
          # Ensure we have at least some topics to process  
          if [ $TOPIC_COUNT -eq 0 ]; then
            echo "âŒ No unique topics found after $MAX_RETRIES retries. This indicates:"
            echo "   - All generated topics were duplicates of recent articles"
            echo "   - API issues preventing topic generation"
            echo "   - Possible topic exhaustion in current search domains"
            echo "âŒ Exiting to prevent infinite processing."
            exit 1
          elif [ $TOPIC_COUNT -lt 3 ]; then
            echo "âš ï¸ Very few unique topics ($TOPIC_COUNT) found. This may indicate topic saturation."
            echo "âš ï¸ Consider adjusting search criteria or reducing posting frequency."
          fi
          
          echo "ğŸ“Š Final unique topics selected: $TOPIC_COUNT/5"
          TOPICS=$(printf '%s\n' "$TOPICS")
          echo "Final topics for processing:"
          echo "$TOPICS"
          
          # Optimized batch article generation (reduce API calls from 5 to 2-3)
          echo "ğŸ“ Batch article generation with integrated Mermaid diagrams..."
          
          # Create Python script for batch processing
          python3 -c "
import sys
import os
import re
from datetime import datetime

# Read topics
with open('_temp/topics_for_loop.txt', 'r', encoding='utf-8') as f:
    all_topics = [line.strip() for line in f if line.strip()]

print(f'ğŸ“Š Processing {len(all_topics)} topics in 2 batch API calls...')

# Split topics into 2 batches for API optimization
batch1 = all_topics[:3] if len(all_topics) >= 3 else all_topics
batch2 = all_topics[3:5] if len(all_topics) > 3 else []

batches = [batch1, batch2] if batch2 else [batch1]

for batch_num, topics_batch in enumerate(batches, 1):
    if not topics_batch:
        continue
        
    print(f'ğŸ“ Processing batch {batch_num}: {len(topics_batch)} articles')
    
    # Create batch prompt
    batch_prompt = f'''WebSearch: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')} AIæ¥­ç•Œã€‚

**ãƒŸãƒƒã‚·ãƒ§ãƒ³:** ä»¥ä¸‹ã®{len(topics_batch)}å€‹ã®è©±é¡Œã«ã¤ã„ã¦ã€ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸå°‚é–€è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

**è¨˜äº‹è¦ä»¶:**
- Webæ¤œç´¢ã§æœ€æ–°æƒ…å ±ã‚’èª¿æŸ»
- ä¼æ¥­åãƒ»è£½å“åãƒ»äººåãƒ»æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ç¢ºã«è¨˜è¼‰
- æŠ€è¡“è€…ãƒ»æŠ•è³‡å®¶å‘ã‘ã®è©³ç´°åˆ†æ
- 3000-4000æ–‡å­—ç¨‹åº¦
- AIã£ã½ã„è¡¨ç¾ã‚’é¿ã‘è‡ªç„¶ãªæ–‡ç« 

**å‡ºåŠ›å½¢å¼ï¼ˆå³å¯†ã«å¾“ã£ã¦ãã ã•ã„ï¼‰:**
=== ARTICLE_START_1 ===
# [è©±é¡Œ1ã®ã‚¿ã‚¤ãƒˆãƒ«]

## æ¦‚è¦ã¨èƒŒæ™¯
[è©³ç´°ãªå†…å®¹]

## è©³ç´°ãªæŠ€è¡“ãƒ»ãƒ“ã‚¸ãƒã‚¹å†…å®¹
[è©³ç´°ãªå†…å®¹]

## å¸‚å ´ãƒ»ç«¶åˆã¸ã®å½±éŸ¿
[è©³ç´°ãªå†…å®¹]

## å›³è§£ãƒ»ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ
**Mermaidãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ:**
```mermaid
[è¨˜äº‹å†…å®¹ã«é©ã—ãŸMermaidå›³è§£ã‚’ç”Ÿæˆ]
// ä¾‹ï¼šãƒ—ãƒ­ã‚»ã‚¹ãƒ•ãƒ­ãƒ¼ã€ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆã€é–¢ä¿‚æ€§å›³ãªã©
// åˆ¶ç´„ï¼šsubgraphã€linkStyleã€font-weightä½¿ç”¨ä¸å¯
// æ¨å¥¨ï¼šåŸºæœ¬ãƒãƒ¼ãƒ‰å½¢çŠ¶[]ã€{}ã€()ã€è‰²åˆ†ã‘styleæ–‡
```

## ä»Šå¾Œã®å±•æœ›
[è©³ç´°ãªå†…å®¹]
=== ARTICLE_END_1 ===

=== ARTICLE_START_2 ===
# [è©±é¡Œ2ã®ã‚¿ã‚¤ãƒˆãƒ«]
[åŒæ§˜ã®æ§‹æˆã§Mermaidå›³è§£ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã‚€]
=== ARTICLE_END_2 ===

**å¯¾è±¡è©±é¡Œ:**'''
    
    for i, topic_line in enumerate(topics_batch, 1):
        topic = re.sub(r'^[0-9]*\. *', '', topic_line)
        batch_prompt += f'''
{i}. {topic}'''
    
    # Write batch prompt to file for gemini CLI
    with open(f'_temp/batch_prompt_{batch_num}.txt', 'w', encoding='utf-8') as f:
        f.write(batch_prompt)
    
    print(f'âœ… Batch {batch_num} prompt prepared')
"
          
          # Execute batch API calls
          BATCH_SUCCESS=0
          for batch_num in 1 2; do
            if [ ! -f "_temp/batch_prompt_${batch_num}.txt" ]; then
              continue
            fi
            
            echo "ğŸš€ Executing batch $batch_num API call..."
            
            if timeout 180 gemini -m "gemini-2.5-flash" < "_temp/batch_prompt_${batch_num}.txt" > "_temp/batch_response_${batch_num}.md" 2>_temp/batch_error_${batch_num}.log; then
              echo "âœ… Batch $batch_num API call successful"
              BATCH_SUCCESS=$((BATCH_SUCCESS + 1))
            else
              echo "âŒ Batch $batch_num failed"
              cat _temp/batch_error_${batch_num}.log 2>/dev/null || echo "No error log"
              
              if grep -q "429\|RESOURCE_EXHAUSTED\|quota" _temp/batch_error_${batch_num}.log 2>/dev/null; then
                echo "ğŸš« Rate limit reached, stopping batch processing"
                break
              fi
            fi
          done
          
          # Process batch responses with integrated Mermaid processing
          python3 -c "
import sys
import os
import re
import base64
import requests
from datetime import datetime
from PIL import Image
import io

generated_count = 0
current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S %z')

def mermaid_to_image_simple(mermaid_code, output_path):
    '''Convert Mermaid to image with simplified syntax'''
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Simplify Mermaid code to avoid 404 errors
        lines = mermaid_code.split('\n')
        simplified_lines = []
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('//'):
                continue
            # Remove complex styles that cause 404s    
            if 'subgraph' in line.lower() or 'linkStyle' in line or 'font-weight' in line:
                continue
            simplified_lines.append(line)
        
        simplified_code = '\n'.join(simplified_lines)
        
        # Convert to image via Mermaid.ink
        encoded_mermaid = base64.b64encode(simplified_code.encode('utf-8')).decode('utf-8')
        mermaid_url = f'https://mermaid.ink/img/{encoded_mermaid}'
        
        if len(mermaid_url) > 1800:
            print(f'âš ï¸ Mermaid URL too long, skipping: {os.path.basename(output_path)}')
            return False
            
        response = requests.get(mermaid_url, timeout=30)
        if response.status_code == 200:
            image = Image.open(io.BytesIO(response.content))
            
            # Convert to JPEG with white background
            if image.mode in ('RGBA', 'LA', 'P'):
                background = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'P':
                    image = image.convert('RGBA')
                background.paste(image, mask=image.split()[-1] if image.mode == 'RGBA' else None)
                image = background
            
            image.save(output_path, 'JPEG', quality=85, optimize=True)
            print(f'âœ… Mermaid image saved: {os.path.basename(output_path)}')
            return True
        else:
            print(f'âŒ Mermaid.ink failed ({response.status_code}): {os.path.basename(output_path)}')
            return False
    except Exception as e:
        print(f'âŒ Mermaid processing error: {e}')
        return False

for batch_num in [1, 2]:
    response_file = f'_temp/batch_response_{batch_num}.md'
    if not os.path.exists(response_file):
        continue
        
    try:
        with open(response_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Extract individual articles using markers
        articles = re.findall(r'=== ARTICLE_START_(\d+) ===(.*?)=== ARTICLE_END_\1 ===', content, re.DOTALL)
        
        for article_idx, article_content in articles:
            article_content = article_content.strip()
            if not article_content:
                continue
                
            # Extract title
            title_match = re.search(r'^# (.+)$', article_content, re.MULTILINE)
            if not title_match:
                continue
                
            title = title_match.group(1).strip()
            generated_count += 1
            
            # Create filename
            slug = re.sub(r'[^a-zA-Z0-9\s]', '', title.lower())[:50]
            slug = re.sub(r'\s+', '-', slug).strip('-')
            filename = f'{datetime.now().strftime(\"%Y-%m-%d\")}-{generated_count}-{slug}.md'
            
            # Extract and process Mermaid diagrams
            mermaid_matches = re.findall(r'```mermaid\s*(.*?)\s*```', article_content, re.DOTALL)
            for i, mermaid_code in enumerate(mermaid_matches, 1):
                mermaid_code = mermaid_code.strip()
                if mermaid_code and not mermaid_code.startswith('[è¨˜äº‹å†…å®¹ã«é©ã—ãŸ'):
                    # Generate Mermaid image
                    mermaid_filename = f'{datetime.now().strftime(\"%Y-%m-%d\")}-{generated_count}-mermaid-{i}.jpg'
                    mermaid_path = f'assets/images/posts/{mermaid_filename}'
                    
                    if mermaid_to_image_simple(mermaid_code, mermaid_path):
                        # Replace Mermaid code block with image reference
                        image_md = f'![{title} - å›³è§£ {i}](/assets/images/posts/{mermaid_filename})'
                        article_content = article_content.replace(f'```mermaid\n{mermaid_code}\n```', image_md)
            
            # Determine category
            category = 'æœ€æ–°å‹•å‘'
            if any(word in title for word in ['ç ”ç©¶', 'è«–æ–‡', 'å­¦ä¼š']):
                category = 'ç ”ç©¶è«–æ–‡'
            elif any(word in title for word in ['æŠ€è¡“', 'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'ä»•çµ„ã¿']):
                category = 'æŠ€è¡“è§£èª¬'
            elif any(word in title for word in ['å°å…¥', 'äº‹ä¾‹', 'æ´»ç”¨']):
                category = 'å®Ÿè£…äº‹ä¾‹'
            elif any(word in title for word in ['å¸‚å ´', 'åˆ†æ', 'äºˆæ¸¬']):
                category = 'æ¥­ç•Œåˆ†æ'
            
            # Create frontmatter
            frontmatter = f'''---
layout: post
title: \"{title}\"
date: {current_date}
categories: [\"{category}\"]
tags: [\"AI\", \"æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹\", \"æŠ€è¡“å‹•å‘\"]
author: \"AIè¨˜äº‹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ \"
excerpt: \"AIæ¥­ç•Œã®æœ€æ–°å‹•å‘ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚\"
reading_time: 8
---

'''
            
            # Write final article file
            with open(f'_temp/temp-{filename}', 'w', encoding='utf-8', errors='ignore') as f:
                f.write(frontmatter + article_content)
            
            print(f'âœ… Generated article {generated_count}: {filename}')
    
    except Exception as e:
        print(f'âŒ Error processing batch {batch_num}: {e}')

print(f'ğŸ“Š Generated {generated_count} articles with integrated Mermaid diagrams')
"
          
          echo "ğŸ“Š Batch article generation completed"

      - name: Semantic article selection
        run: |
          echo "ğŸ§  Starting semantic analysis..."
          
          # Get existing titles
          if ls _posts/*.md 1> /dev/null 2>&1; then
            find _posts -name "*.md" -mtime -2 -exec grep -h "^title:" {} \; 2>/dev/null | head -15 > _temp/existing_titles.txt
          else
            touch _temp/existing_titles.txt
          fi
          
          # Create Python script for semantic analysis using echo statements
          echo "import os" > _temp/semantic_selector.py
          echo "import glob" >> _temp/semantic_selector.py
          echo "from sentence_transformers import SentenceTransformer" >> _temp/semantic_selector.py
          echo "from sklearn.metrics.pairwise import cosine_similarity" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "print('Loading multilingual sentence transformer model...')" >> _temp/semantic_selector.py
          echo "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "def get_embedding(text):" >> _temp/semantic_selector.py
          echo "    return model.encode([text])" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "def extract_title(filepath):" >> _temp/semantic_selector.py
          echo "    try:" >> _temp/semantic_selector.py
          echo "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:" >> _temp/semantic_selector.py
          echo "            for line in f:" >> _temp/semantic_selector.py
          echo "                if line.startswith('title:'):" >> _temp/semantic_selector.py
          echo "                    return line.replace('title:', '').strip().strip('\"')" >> _temp/semantic_selector.py
          echo "    except Exception as e:" >> _temp/semantic_selector.py
          echo "        print(f'Error reading {filepath}: {e}')" >> _temp/semantic_selector.py
          echo "    return ''" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "existing_titles = []" >> _temp/semantic_selector.py
          echo "if os.path.exists('_temp/existing_titles.txt'):" >> _temp/semantic_selector.py
          echo "    with open('_temp/existing_titles.txt', 'r') as f:" >> _temp/semantic_selector.py
          echo "        existing_titles = [line.strip().replace('title:', '').strip().strip('\"') for line in f.readlines() if line.strip()]" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "temp_articles = glob.glob('_temp/temp-*.md')" >> _temp/semantic_selector.py
          echo "published_count = 0" >> _temp/semantic_selector.py
          echo "print(f'Evaluating {len(temp_articles)} articles...')" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "for filepath in temp_articles:" >> _temp/semantic_selector.py
          echo "    title = extract_title(filepath)" >> _temp/semantic_selector.py
          echo "    if not title:" >> _temp/semantic_selector.py
          echo "        continue" >> _temp/semantic_selector.py
          echo "    max_similarity = 0.0" >> _temp/semantic_selector.py
          echo "    if existing_titles:" >> _temp/semantic_selector.py
          echo "        title_embedding = get_embedding(title)" >> _temp/semantic_selector.py
          echo "        for existing_title in existing_titles:" >> _temp/semantic_selector.py
          echo "            if existing_title:" >> _temp/semantic_selector.py
          echo "                existing_embedding = get_embedding(existing_title)" >> _temp/semantic_selector.py
          echo "                similarity = cosine_similarity(title_embedding.reshape(1, -1), existing_embedding.reshape(1, -1))[0][0]" >> _temp/semantic_selector.py
          echo "                max_similarity = max(max_similarity, similarity)" >> _temp/semantic_selector.py
          echo "    is_duplicate = max_similarity > 0.75" >> _temp/semantic_selector.py
          echo "    if not is_duplicate:" >> _temp/semantic_selector.py
          echo "        final_name = os.path.basename(filepath).replace('temp-', '')" >> _temp/semantic_selector.py
          echo "        os.rename(filepath, f'_posts/{final_name}')" >> _temp/semantic_selector.py
          echo "        print(f'Published: {final_name}')" >> _temp/semantic_selector.py
          echo "        published_count += 1" >> _temp/semantic_selector.py
          echo "    else:" >> _temp/semantic_selector.py
          echo "        print(f'Skipped duplicate: {title[:50]}... (similarity: {max_similarity:.3f})')" >> _temp/semantic_selector.py
          echo "print(f'Published {published_count} unique articles')" >> _temp/semantic_selector.py
          
          # Run the semantic selector
          python3 _temp/semantic_selector.py

      - name: Improve article quality with textlint
        run: |
          echo "ğŸ“ Improving article quality with textlint..."
          
          # Run textlint on all generated articles with AI writing detection
          for article in _posts/*.md; do
            if [ -f "$article" ]; then
              echo "ğŸ” Checking: $(basename "$article")"
              
              # Run textlint with detailed output
              if ! textlint "$article"; then
                echo "âš ï¸ TextLint issues found in $(basename "$article")"
              fi
              
              # Fix common AI-generated patterns and issues
              sed -i 's/ã€ã€/ã€/g' "$article"
              sed -i 's/ã€‚ã€‚/ã€‚/g' "$article"
              # Remove common AI metadata patterns
              sed -i '/^AI ã«ã‚ˆã£ã¦ç”Ÿæˆ/d' "$article"
              sed -i '/^ã“ã®è¨˜äº‹ã¯ AI ã§ç”Ÿæˆ/d' "$article"
              sed -i '/^â€» ã“ã®è¨˜äº‹ã¯ AI/d' "$article"
              # Remove excessive emphasis
              sed -i 's/\*\*\([^*]*\)\*\*/\1/g' "$article" | head -c 0
              # Clean up list formatting
              sed -i 's/^- \*\*/- /g' "$article"
              
              echo "âœ… Improved: $(basename "$article")"
            fi
          done

      - name: Generate images for articles
        env:
          HUGGINGFACE_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY2 }}
        run: |
          export HUGGINGFACE_TOKEN="$HUGGINGFACE_TOKEN"
          export GEMINI_API_KEY="$GEMINI_API_KEY"
          
          echo "ğŸ¨ Generating images for published articles..."
          # Install required dependencies
          pip install requests pillow
          
          # 1. Generate featured images for articles  
          python3 scripts/image_generator.py
          
          # 2. Mermaid diagrams now integrated into article generation (0 additional API calls)
          echo "âœ… Mermaid diagrams integrated into batch article generation (0 additional API calls)"

      - name: Setup Ruby and Jekyll
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.1'
          bundler-cache: true

      - name: Build and Deploy
        run: |
          bundle install
          bundle exec jekyll build

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './_site'

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4

      - name: Commit generated articles
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          if [ -n "$(git status --porcelain _posts/)" ]; then
            ARTICLE_COUNT=$(ls _posts/$(date +%Y-%m-%d)-*.md 2>/dev/null | wc -l)
            git add _posts/
            git commit -m "ğŸ¤– Add $ARTICLE_COUNT unique AI articles - $(date +%Y-%m-%d)"
            git push
            echo "âœ… Committed $ARTICLE_COUNT new articles"
          else
            echo "â„¹ï¸  No new articles to commit"
          fi