name: AI Article Generation & Publishing Pipeline

on:
  schedule:
    - cron: '15 */8 * * *'  # 8ÊôÇÈñì„Åî„Å®
  workflow_dispatch:

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          npm install -g @google/gemini-cli
          pip install transformers torch sentence-transformers scikit-learn numpy requests
          mkdir -p _temp

      - name: Generate topics and articles
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          export GEMINI_API_KEY="$GEMINI_API_KEY"
          
          echo "üéØ Generating AI topics..."
          gemini -m "gemini-2.5-flash" --prompt "WebSearch: AIÊ•≠Áïå ÊúÄÊñ∞„Éã„É•„Éº„Çπ „Éà„É¨„É≥„Éâ Ë©±È°å 2025„ÄÇAIÊ•≠Áïå„ÅßÊ≥®ÁõÆ„Åï„Çå„Å¶„ÅÑ„ÇãÊúÄÊñ∞„ÅÆË©±È°å„Éª„Éà„É¨„É≥„Éâ„ÇíWebÊ§úÁ¥¢„ÅßË™øÊüª„Åó„ÄÅ‰ª•‰∏ã„ÅÆÂΩ¢Âºè„Åß10ÂÄã„ÅÆÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°å„Çí„É™„Çπ„Éà„Ç¢„ÉÉ„Éó„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂêÑË©±È°å„ÅØ1Ë°å„Åß„ÄÅÁ∞°ÊΩî„Å´Ë°®Áèæ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰æã: 1. OpenAI„ÅÆÊñ∞„É¢„Éá„É´Áô∫Ë°®, 2. Google DeepMind„ÅÆÁ†îÁ©∂ÊàêÊûú, 3. ÁîüÊàêAI„ÅÆ‰ºÅÊ•≠Â∞éÂÖ•‰∫ã‰æã, 4. AIË¶èÂà∂Ê≥ïÊ°à„ÅÆÂãïÂêë, 5. Ëá™ÂãïÈÅãËª¢ÊäÄË°ì„ÅÆÈÄ≤Â±ï...ÂÆüÈöõ„ÅÆÊ§úÁ¥¢ÁµêÊûú„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅÂÖ∑‰ΩìÁöÑ„Å™‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„Éª‰∫∫Âêç„ÇíÂê´„ÇÅ„ÅüÁèæÂÆüÁöÑ„Å™Ë©±È°å„Çí10ÂÄã„É™„Çπ„Éà„Ç¢„ÉÉ„Éó„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ" > _temp/topics.txt
          
          echo "Generated topics:"
          head -15 _temp/topics.txt
          
          # Extract and generate articles
          TOPICS=$(grep -E "^[0-9]+\." _temp/topics.txt | head -10)
          echo "üìù Processing topics for article generation..."
          echo "$TOPICS"
          
          if [ -z "$TOPICS" ]; then
            echo "‚ùå No topics generated, using fallback topics"
            echo "1. OpenAI ChatGPT-5„ÅÆÊ©üËÉΩÂêë‰∏ä„Å®„Éì„Ç∏„Éç„ÇπÂøúÁî®" > _temp/fallback_topics.txt
            echo "2. Google Gemini 2.0„ÅÆÁîªÂÉèË™çË≠òÊäÄË°ìÂêë‰∏ä" >> _temp/fallback_topics.txt
            echo "3. Microsoft Copilot for 365„ÅÆ‰ºÅÊ•≠Â∞éÂÖ•Êã°Â§ß" >> _temp/fallback_topics.txt
            echo "4. AIÁîªÂÉèÁîüÊàêÊäÄË°ì„ÅÆËëó‰ΩúÊ®©ÂïèÈ°å„Å®Ê≥ïÁöÑÂØæÂøú" >> _temp/fallback_topics.txt
            echo "5. Ëá™ÂãïÈÅãËª¢AIÊäÄË°ì„ÅÆ2025Âπ¥ÂïÜÁî®Âåñ‰∫àÊ∏¨" >> _temp/fallback_topics.txt
            echo "6. ÂåªÁôÇË®∫Êñ≠AI„ÅÆÁ≤æÂ∫¶Âêë‰∏ä„Å®Ë¶èÂà∂ÂãïÂêë" >> _temp/fallback_topics.txt
            TOPICS=$(cat _temp/fallback_topics.txt)
          fi
          
          echo "Final topics for processing:"
          echo "$TOPICS"
          
          # Generate articles for all topics
          TOPIC_INDEX=1
          GENERATED_COUNT=0
          
          while IFS= read -r TOPIC_LINE; do
            if [ -z "$TOPIC_LINE" ]; then
              continue
            fi
            
            TOPIC=$(echo "$TOPIC_LINE" | sed 's/^[0-9]*\. *//')
            echo "üìù Generating article $TOPIC_INDEX: $TOPIC"
            
            # Category determination
            CATEGORY="ÊúÄÊñ∞ÂãïÂêë"
            if [[ "$TOPIC" =~ (Á†îÁ©∂|Ë´ñÊñá|Â≠¶‰ºö) ]]; then
              CATEGORY="Á†îÁ©∂Ë´ñÊñá"
            elif [[ "$TOPIC" =~ (ÊäÄË°ì|„Ç¢„É´„Ç¥„É™„Ç∫„É†|‰ªïÁµÑ„Åø) ]]; then
              CATEGORY="ÊäÄË°ìËß£Ë™¨"
            elif [[ "$TOPIC" =~ (Â∞éÂÖ•|‰∫ã‰æã|Ê¥ªÁî®) ]]; then
              CATEGORY="ÂÆüË£Ö‰∫ã‰æã"
            elif [[ "$TOPIC" =~ (Â∏ÇÂ†¥|ÂàÜÊûê|‰∫àÊ∏¨) ]]; then
              CATEGORY="Ê•≠ÁïåÂàÜÊûê"
            fi
            
            # Generate article
            PROMPT="WebSearch: $(date '+%YÂπ¥%mÊúà%dÊó•') $TOPIC„ÄÇ„Äå$TOPIC„Äç„Å´„Å§„ÅÑ„Å¶„ÄÅWebÊ§úÁ¥¢„ÅßÊúÄÊñ∞ÊÉÖÂ†±„ÇíË™øÊüª„Åó„ÄÅALLFORCES AIÊÉÖÂ†±„É°„Éá„Ç£„Ç¢Âêë„Åë„ÅÆÂ∞ÇÈñÄË®ò‰∫ã„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇË¶Å‰ª∂ÔºöÂÆüÈöõ„ÅÆÊúÄÊñ∞ÊÉÖÂ†±„Å´Âü∫„Å•„ÅèÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ„ÄÅ‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„Éª‰∫∫Âêç„ÉªÊï∞ÂÄ§„Éá„Éº„Çø„ÇíÊ≠£Á¢∫„Å´Ë®òËºâ„ÄÅÊäÄË°ìËÄÖ„ÉªÊäïË≥áÂÆ∂Âêë„Åë„ÅÆË©≥Á¥∞„Å™ÂàÜÊûê„ÄÅ3000-4000ÊñáÂ≠óÁ®ãÂ∫¶„ÄÅMarkdownÂΩ¢Âºè„ÅßÂá∫Âäõ„ÄÇÊßãÊàêÔºö# $TOPIC ## Ê¶ÇË¶Å„Å®ËÉåÊôØ ## Ë©≥Á¥∞„Å™ÊäÄË°ì„Éª„Éì„Ç∏„Éç„ÇπÂÜÖÂÆπ ## Â∏ÇÂ†¥„ÉªÁ´∂Âêà„Å∏„ÅÆÂΩ±Èüø ## ‰ªäÂæå„ÅÆÂ±ïÊúõ„ÄÇÂ∞ÇÈñÄÊÄß„Å®‰ø°È†ºÊÄß„ÇíÈáçË¶ñ„Åó„ÅüË®ò‰∫ã„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
            
            gemini -m "gemini-2.5-flash" --prompt "$PROMPT" > "_temp/article-${TOPIC_INDEX}.md"
            
            if [ -s "_temp/article-${TOPIC_INDEX}.md" ]; then
              TITLE=$(echo "$TOPIC" | cut -c1-80)
              SLUG=$(echo "$TOPIC" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-zA-Z0-9„ÅÇ-„Çì„Ç¢-„É≥‰∏Ä-ÈæØ]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g' | cut -c1-50)
              FILENAME="$(date +%Y-%m-%d)-${TOPIC_INDEX}-${SLUG}.md"
              
              # Store in temp for processing
              echo "---" > "_temp/temp-${FILENAME}"
              echo "layout: post" >> "_temp/temp-${FILENAME}"
              echo "title: \"$TITLE\"" >> "_temp/temp-${FILENAME}"
              echo "date: $(date +%Y-%m-%d\ %H:%M:%S\ %z)" >> "_temp/temp-${FILENAME}"
              echo "categories: [\"$CATEGORY\"]" >> "_temp/temp-${FILENAME}"
              echo "tags: [\"AI\", \"ÊúÄÊñ∞„Éã„É•„Éº„Çπ\", \"ÊäÄË°ìÂãïÂêë\"]" >> "_temp/temp-${FILENAME}"
              echo "author: \"AIË®ò‰∫ãÁîüÊàê„Ç∑„Çπ„ÉÜ„É†\"" >> "_temp/temp-${FILENAME}"
              echo "excerpt: \"AIÊ•≠Áïå„ÅÆÊúÄÊñ∞ÂãïÂêë„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèËß£Ë™¨„Åó„Åæ„Åô„ÄÇ\"" >> "_temp/temp-${FILENAME}"
              echo "reading_time: 8" >> "_temp/temp-${FILENAME}"
              echo "---" >> "_temp/temp-${FILENAME}"
              
              sed '1d' "_temp/article-${TOPIC_INDEX}.md" | sed '/^```markdown$/d' | sed '/^```$/d' >> "_temp/temp-${FILENAME}"
              
              echo "‚úÖ Generated: temp-${FILENAME}"
              GENERATED_COUNT=$((GENERATED_COUNT + 1))
            else
              echo "‚ùå Failed to generate: $TOPIC"
            fi
            
            TOPIC_INDEX=$((TOPIC_INDEX + 1))
          done <<< "$TOPICS"
          
          echo "üìä Generated $GENERATED_COUNT articles"

      - name: Semantic article selection
        run: |
          echo "üß† Starting semantic analysis..."
          
          # Get existing titles
          if ls _posts/*.md 1> /dev/null 2>&1; then
            find _posts -name "*.md" -mtime -2 -exec grep -h "^title:" {} \; 2>/dev/null | head -15 > _temp/existing_titles.txt
          else
            touch _temp/existing_titles.txt
          fi
          
          # Create Python script for semantic analysis using echo statements
          echo "import os" > _temp/semantic_selector.py
          echo "import glob" >> _temp/semantic_selector.py
          echo "from transformers import AutoTokenizer, AutoModel" >> _temp/semantic_selector.py
          echo "import torch" >> _temp/semantic_selector.py
          echo "from sklearn.metrics.pairwise import cosine_similarity" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "print('Loading Japanese BERT model...')" >> _temp/semantic_selector.py
          echo "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'" >> _temp/semantic_selector.py
          echo "tokenizer = AutoTokenizer.from_pretrained(model_name)" >> _temp/semantic_selector.py
          echo "model = AutoModel.from_pretrained(model_name)" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "def get_embedding(text):" >> _temp/semantic_selector.py
          echo "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)" >> _temp/semantic_selector.py
          echo "    with torch.no_grad():" >> _temp/semantic_selector.py
          echo "        outputs = model(**inputs)" >> _temp/semantic_selector.py
          echo "    return outputs.last_hidden_state.mean(dim=1).numpy()" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "def extract_title(filepath):" >> _temp/semantic_selector.py
          echo "    with open(filepath, 'r', encoding='utf-8') as f:" >> _temp/semantic_selector.py
          echo "        for line in f:" >> _temp/semantic_selector.py
          echo "            if line.startswith('title:'):" >> _temp/semantic_selector.py
          echo "                return line.replace('title:', '').strip().strip('\"')" >> _temp/semantic_selector.py
          echo "    return ''" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "existing_titles = []" >> _temp/semantic_selector.py
          echo "if os.path.exists('_temp/existing_titles.txt'):" >> _temp/semantic_selector.py
          echo "    with open('_temp/existing_titles.txt', 'r') as f:" >> _temp/semantic_selector.py
          echo "        existing_titles = [line.strip().replace('title:', '').strip().strip('\"') for line in f.readlines() if line.strip()]" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "temp_articles = glob.glob('_temp/temp-*.md')" >> _temp/semantic_selector.py
          echo "published_count = 0" >> _temp/semantic_selector.py
          echo "print(f'Evaluating {len(temp_articles)} articles...')" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "for filepath in temp_articles:" >> _temp/semantic_selector.py
          echo "    title = extract_title(filepath)" >> _temp/semantic_selector.py
          echo "    if not title:" >> _temp/semantic_selector.py
          echo "        continue" >> _temp/semantic_selector.py
          echo "    max_similarity = 0.0" >> _temp/semantic_selector.py
          echo "    if existing_titles:" >> _temp/semantic_selector.py
          echo "        title_embedding = get_embedding(title)" >> _temp/semantic_selector.py
          echo "        for existing_title in existing_titles:" >> _temp/semantic_selector.py
          echo "            if existing_title:" >> _temp/semantic_selector.py
          echo "                existing_embedding = get_embedding(existing_title)" >> _temp/semantic_selector.py
          echo "                similarity = cosine_similarity(title_embedding.reshape(1, -1), existing_embedding.reshape(1, -1))[0][0]" >> _temp/semantic_selector.py
          echo "                max_similarity = max(max_similarity, similarity)" >> _temp/semantic_selector.py
          echo "    is_duplicate = max_similarity > 0.75" >> _temp/semantic_selector.py
          echo "    if not is_duplicate:" >> _temp/semantic_selector.py
          echo "        final_name = os.path.basename(filepath).replace('temp-', '')" >> _temp/semantic_selector.py
          echo "        os.rename(filepath, f'_posts/{final_name}')" >> _temp/semantic_selector.py
          echo "        print(f'Published: {final_name}')" >> _temp/semantic_selector.py
          echo "        published_count += 1" >> _temp/semantic_selector.py
          echo "    else:" >> _temp/semantic_selector.py
          echo "        print(f'Skipped duplicate: {title[:50]}... (similarity: {max_similarity:.3f})')" >> _temp/semantic_selector.py
          echo "print(f'Published {published_count} unique articles')" >> _temp/semantic_selector.py
          
          # Run the semantic selector
          python3 _temp/semantic_selector.py

      - name: Setup Ruby and Jekyll
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.1'
          bundler-cache: true

      - name: Build and Deploy
        run: |
          bundle install
          bundle exec jekyll build

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './_site'

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4

      - name: Commit generated articles
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          if [ -n "$(git status --porcelain _posts/)" ]; then
            ARTICLE_COUNT=$(ls _posts/$(date +%Y-%m-%d)-*.md 2>/dev/null | wc -l)
            git add _posts/
            git commit -m "ü§ñ Add $ARTICLE_COUNT AI articles - $(date +%Y-%m-%d)"
            git push
            echo "‚úÖ Committed $ARTICLE_COUNT new articles"
          else
            echo "‚ÑπÔ∏è  No new articles to commit"
          fi