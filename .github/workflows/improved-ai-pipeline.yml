name: AI Article Generation & Publishing Pipeline - Quality Enhanced

on:
  schedule:
    - cron: '15 */4 * * *'  # Every 4 hours
  workflow_dispatch:

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
        with:
          fetch-depth: 0

      - name: Sync with latest changes
        run: |
          echo "üîÑ Syncing with latest remote changes..."
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git pull origin main || echo "‚ö†Ô∏è No remote changes to pull"

      - name: Setup Node.js
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4
        with:
          node-version: '20'

      - name: Setup Python
        uses: actions/setup-python@7f4fc3e22c37d6ff65e88745f38bd3157c663f7c # v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          npm install -g textlint textlint-rule-preset-japanese textlint-rule-preset-ja-technical-writing @textlint-ja/textlint-rule-preset-ai-writing
          pip install -r requirements.txt
          mkdir -p _temp _posts assets/images/posts scripts
          chmod +x scripts/generate_content.py

      # STAGE 1: Simplified and Reliable Topic Generation
      - name: Generate high-quality topics
        shell: bash
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY2 }}
        run: |
          export GEMINI_API_KEY="$GEMINI_API_KEY"
          
          echo "üéØ Generating focused AI topics..."

          # API key verification
          if [ -z "$GEMINI_API_KEY" ]; then
            echo "‚ùå GEMINI_API_KEY is not set"
            exit 1
          fi
          
          # Simple and reliable topic generation prompt
          # Enhanced topic prompt with diversity focus and recent content filtering

          # Determine today's target categories (rotate to ensure diversity)
          DAY_OF_WEEK=$(date +%u)
          case $DAY_OF_WEEK in
            1) TARGET_CATS="AIÊäÄË°ì„Ç¨„Ç§„Éâ„ÄÅÊ•≠ÁïåÂà•AIÊ¥ªÁî®„ÄÅAIÊúÄÊñ∞„Éã„É•„Éº„Çπ" ;;
            2) TARGET_CATS="Â∞éÂÖ•‰∫ã‰æã„ÄÅAIÊúÄÊñ∞„Éã„É•„Éº„Çπ„ÄÅAIÂ∞éÂÖ•Êà¶Áï•" ;;
            3) TARGET_CATS="AIÊäÄË°ì„Ç¨„Ç§„Éâ„ÄÅÁ†îÁ©∂Ë´ñÊñá„ÄÅÊ•≠ÁïåÂà•AIÊ¥ªÁî®" ;;
            4) TARGET_CATS="AIÊúÄÊñ∞„Éã„É•„Éº„Çπ„ÄÅÂ∞éÂÖ•‰∫ã‰æã„ÄÅAIÊäÄË°ì„Ç¨„Ç§„Éâ" ;;
            5) TARGET_CATS="Ê•≠ÁïåÂà•AIÊ¥ªÁî®„ÄÅAIÂ∞éÂÖ•Êà¶Áï•„ÄÅAIÊúÄÊñ∞„Éã„É•„Éº„Çπ" ;;
            *) TARGET_CATS="AIÊäÄË°ì„Ç¨„Ç§„Éâ„ÄÅAIÊúÄÊñ∞„Éã„É•„Éº„Çπ„ÄÅÊ•≠ÁïåÂà•AIÊ¥ªÁî®" ;;
          esac

          RECENT_TITLES=$(find _posts -name "$(date +%Y-%m)*" -exec basename {} \; 2>/dev/null | head -10 | tr '\n' ' ')
          TOPIC_PROMPT="WebSearch: AIÊ•≠Áïå ÊúÄÊñ∞„Éã„É•„Éº„Çπ $(date '+%YÂπ¥%mÊúà%dÊó•'). „ÄêÈáçË¶Å„ÄëÈÅéÂéªË®ò‰∫ã„Å®„ÅÆÈáçË§á„ÇíÈÅø„Åë„ÄÅ‰ª•‰∏ã„ÅÆÊúÄËøë„ÅÆ„Çø„Ç§„Éà„É´„Å®Áï∞„Å™„ÇãÊñ∞„Åó„ÅÑË©±È°å„ÇíÁîüÊàê: $RECENT_TITLES „ÄêË¶ÅÊ±Ç„ÄëAIÊ•≠Áïå„ÅÆÊúÄÊñ∞„ÉªÁã¨Ëá™Ë©±È°å„Çí3ÂÄãÁîüÊàê„ÄÇ„Äê„Ç´„ÉÜ„Ç¥„É™ÊåáÂÆö„Äë‰ªäÊó•„ÅÆ3Ë®ò‰∫ã„ÅØ‰ª•‰∏ã„ÅÆ„Ç´„ÉÜ„Ç¥„É™„Åß1Êú¨„Åö„Å§ÁîüÊàê„Åô„Çã„Åì„Å®: ${TARGET_CATS}„ÄÇÂêÑ„Éà„Éî„ÉÉ„ÇØ„ÅÆÂÖàÈ†≠„Å´[„Ç´„ÉÜ„Ç¥„É™Âêç]„Çí‰ªòË®ò„ÄÇ„ÄêÂøÖÈ†àÊù°‰ª∂„Äë1. Áï∞„Å™„Çã‰ºÅÊ•≠„ÉªÂàÜÈáé„ÉªËßíÂ∫¶„Åã„ÇâÈÅ∏ÊäûÔºàOpenAI„ÄÅNVIDIA„ÄÅGoogle„ÄÅMicrosoft„ÄÅMeta„ÄÅAmazon„ÄÅApple„ÄÅAnthropic„ÄÅ‰∏≠ÂõΩAI‰ºÅÊ•≠„ÄÅÊñ∞Ëàà‰ºÅÊ•≠„ÄÅË¶èÂà∂„ÉªÊîøÁ≠ñ„ÄÅÊ•≠ÁïåÂãïÂêëÁ≠âÔºâ 2. ÂÖ∑‰ΩìÁöÑ„Å™‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„ÉªÊï∞ÂÄ§„ÇíÂê´„ÇÄ 3. 30ÊñáÂ≠ó‰ª•ÂÜÖ„ÅÆ„Çø„Ç§„Éà„É´ 4. ÊäïË≥áÂÆ∂„ÉªÊäÄË°ìËÄÖ„ÅåÈñ¢ÂøÉ„ÇíÊåÅ„Å§ÂÜÖÂÆπ 5. Áï™Âè∑‰ªò„Åç„É™„Çπ„ÉàÂΩ¢Âºè „ÄêÂ§öÊßòÊÄßÈáçË¶ñ„ÄëÂêå„Åò‰ºÅÊ•≠„ÉªÂêå„Åò„Éà„Éî„ÉÉ„ÇØÔºàGPT„ÄÅNVIDIAÁ≠âÔºâ„ÅÆÈáçË§á„ÇíÈÅø„Åë„Çã„ÄÇ„ÄêÂá∫Âäõ‰æã„Äë1. [AIÊäÄË°ì„Ç¨„Ç§„Éâ] RAG 2.0„ÅÆ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÄÅ‰Ωï„ÅåÂ§â„Çè„Å£„Åü„ÅÆ„Åã 2. [Ê•≠ÁïåÂà•AIÊ¥ªÁî®] Ë£ΩÈÄ†Ê•≠„ÅÆAIÂìÅË≥™Ê§úÊüª„ÄÅÊ≠©Áïô„Åæ„Çä15%ÊîπÂñÑ„ÅÆË£èÂÅ¥ 3. [AIÊúÄÊñ∞„Éã„É•„Éº„Çπ] Anthropic Claude 4.5„ÄÅ„Ç≥„Éº„ÉâÁîüÊàêÁ≤æÂ∫¶40%Âêë‰∏ä"

          # Enhanced API call with comprehensive validation
          echo "üîÑ Attempting topic generation (with retry)..."
          TOPIC_GENERATION_SUCCESS=false

          for api_retry in 1 2; do
            echo "üì° API attempt $api_retry for topic generation"

            # Use Python script for API calls (handles model rotation internally)
            if python3 scripts/generate_content.py "$TOPIC_PROMPT" _temp/topics.txt 2>_temp/topic_generation_error.log; then
              echo "‚úÖ API call completed successfully"

              # Comprehensive content validation
              if [ -s "_temp/topics.txt" ]; then
                echo "üìÑ Topics file created with content:"
                cat _temp/topics.txt

                # Check for error messages in response
                if grep -qi "error\|failed\|unable\|quota\|limit" _temp/topics.txt 2>/dev/null; then
                  echo "‚ö†Ô∏è API response contains error messages"
                  echo "Response content:"
                  cat _temp/topics.txt
                else
                  # Safe topic counting with file existence check
                  TOPIC_COUNT=0
                  if [ -s "_temp/topics.txt" ]; then
                    TOPIC_COUNT=$(grep -c "^[0-9]\+\." _temp/topics.txt 2>/dev/null || echo "0")
                  fi

                  # Validate topic count is a number and > 0
                  if [[ "$TOPIC_COUNT" =~ ^[0-9]+$ ]] && [ "$TOPIC_COUNT" -gt 0 ]; then
                    echo "üìä Generated $TOPIC_COUNT valid topics"
                    TOPIC_GENERATION_SUCCESS=true
                    break  # Break out of retry loop
                  else
                    echo "‚ö†Ô∏è No valid numbered topics found in response"
                    echo "Expected format: '1. Topic title'"
                  fi
                fi
              else
                echo "‚ùå Topics file is empty or not created"
                if [ -f "_temp/topic_generation_error.log" ]; then
                  echo "Error log:"
                  cat _temp/topic_generation_error.log
                fi
              fi
            else
              echo "‚ùå API call failed with exit code $?"
              if [ -f "_temp/topic_generation_error.log" ]; then
                echo "Error details:"
                cat _temp/topic_generation_error.log
              fi
            fi
            
            if [ "$TOPIC_GENERATION_SUCCESS" = "false" ] && [ "$api_retry" -lt 2 ]; then
              echo "üîÑ Retrying in 15 seconds..."
              sleep 15
            fi
          done
          
          if [ "$TOPIC_GENERATION_SUCCESS" = "false" ]; then
            echo "‚ùå Failed to generate valid topics after 2 attempts"
            echo "üîß Attempting fallback with simpler prompt..."
            
            # Fallback with simpler prompt (Python script handles model rotation)
            SIMPLE_PROMPT="Generate 3 AI industry news topics in this format: 1. Topic 2. Topic 3. Topic. Each topic should be under 30 characters and mention specific companies."

            echo "üîÑ Fallback attempt with simpler prompt"
            if python3 scripts/generate_content.py "$SIMPLE_PROMPT" _temp/topics_fallback.txt 2>/dev/null; then
              if [ -s "_temp/topics_fallback.txt" ]; then
                mv _temp/topics_fallback.txt _temp/topics.txt
                echo "‚úÖ Fallback topics generated"
                cat _temp/topics.txt
              fi
            fi
            
            # Final validation
            FINAL_TOPIC_COUNT=0
            if [ -s "_temp/topics.txt" ]; then
              FINAL_TOPIC_COUNT=$(grep -c "^[0-9]\+\." _temp/topics.txt 2>/dev/null || echo "0")
            fi
            
            if [[ "$FINAL_TOPIC_COUNT" =~ ^[0-9]+$ ]] && [ "$FINAL_TOPIC_COUNT" -gt 0 ]; then
              echo "üìä Final validation: $FINAL_TOPIC_COUNT topics available"
            else
              echo "üíÄ Complete failure: No valid topics could be generated"
              exit 1
            fi
          fi

      # STAGE 2: Quality-Focused Article Generation
      - name: Generate high-quality articles
        shell: bash
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY2 }}
        run: |
          echo "üìù Generating professional articles..."
          
          # Extract topics and save as temp files for duplicate checking
          grep -E "^[0-9]+\." _temp/topics.txt | head -3 > _temp/selected_topics.txt
          
          # Debug: Show extracted topics
          echo "üìã Extracted topics for processing:"
          cat _temp/selected_topics.txt
          AVAILABLE_TOPICS=$(wc -l < _temp/selected_topics.txt)
          echo "üìä Available topics: $AVAILABLE_TOPICS"
          
          # Prepare existing titles for duplicate checking
          echo "üîç Preparing duplicate check data..."
          find _posts -name "*.md" -mtime -3 | head -20 | while read -r post; do
            if [ -f "$post" ]; then
              grep "^title:" "$post" 2>/dev/null || true
            fi
          done > _temp/existing_titles.txt || true
          
          echo "üìù Existing titles for duplicate checking:"
          head -5 _temp/existing_titles.txt || echo "No existing titles found"
          
          # Use robust for-loop instead of while-loop to ensure all topics are processed
          GENERATED_COUNT=0
          FAILED_COUNT=0
          
          for topic_number in 1 2 3; do
            TOPIC_LINE=$(sed -n "${topic_number}p" _temp/selected_topics.txt 2>/dev/null)
            
            if [ -z "$TOPIC_LINE" ]; then
              echo "‚ö†Ô∏è No topic found at position $topic_number"
              continue
            fi
            
            TOPIC=$(echo "$TOPIC_LINE" | sed 's/^[0-9]*\. *//')

            # Extract category hint from topic (format: [„Ç´„ÉÜ„Ç¥„É™Âêç] „Éà„Éî„ÉÉ„ÇØ)
            TOPIC_CATEGORY=""
            TOPIC_AUTHOR="ALLFORCESÁ∑®ÈõÜÈÉ®"
            if echo "$TOPIC" | grep -q '^\['; then
              TOPIC_CATEGORY=$(echo "$TOPIC" | sed 's/^\[\([^]]*\)\].*/\1/')
              TOPIC=$(echo "$TOPIC" | sed 's/^\[[^]]*\] *//')
            fi

            # Category-specific persona and structure
            case "$TOPIC_CATEGORY" in
              "AIÊäÄË°ì„Ç¨„Ç§„Éâ")
                PERSONA="„ÅÇ„Å™„Åü„ÅØ„ÄÅÂ§ßÊâã„ÉÜ„ÉÉ„ÇØ‰ºÅÊ•≠„ÅßML„Ç®„É≥„Ç∏„Éã„Ç¢„Å®„Åó„Å¶10Âπ¥„ÅÆÁµåÈ®ì„ÇíÊåÅ„Å§ÊäÄË°ì„É©„Ç§„Çø„Éº„Åß„Åô„ÄÇLLM„Åã„ÇâMLOps„Åæ„ÅßÂπÖÂ∫É„ÅÑÂÆüË£ÖÁµåÈ®ì„Åå„ÅÇ„Çä„ÄÅË§áÈõë„Å™ÊäÄË°ì„ÇíÂÆüË∑µÁöÑ„Å™„Ç≥„Éº„Éâ‰æã„Çí‰∫§„Åà„Å¶Ëß£Ë™¨„Åô„Çã„ÅÆ„ÅåÂæóÊÑè„Åß„Åô„ÄÇÁêÜË´ñ„Å†„Åë„Åß„Å™„Åè„ÄåÂÆüÈöõ„Å´Âãï„Åã„Åó„Å¶„Åø„Åü„ÄçË¶ñÁÇπ„ÇíÂ§ßÂàá„Å´„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
                STRUCTURE="„ÄêË®ò‰∫ãÊßãÊàê„Äë1.ÊäÄË°ì„ÅÆÊ¶ÇË¶Å„Å®ËÉåÊôØÔºà„Å™„Åú‰ªä„Åì„ÅÆÊäÄË°ì„ÅãÔºâ2.„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Ë©≥Á¥∞ÔºàÂõ≥Ëß£ÁöÑË™¨ÊòéÔºâ3.ÂÆüË£Ö„ÅÆ„Éù„Ç§„É≥„ÉàÔºàÊì¨‰ºº„Ç≥„Éº„Éâ„ÇÑ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„ÇπÔºâ4.„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊØîËºÉÔºà„Éô„É≥„ÉÅ„Éû„Éº„ÇØÊï∞ÂÄ§Ôºâ5.Â∞éÂÖ•ÊôÇ„ÅÆÊ≥®ÊÑèÁÇπ„Å®Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó"
                TOPIC_AUTHOR="ALLFORCES„ÉÜ„ÇØ„Éã„Ç´„É´„ÉÅ„Éº„É†"
                ;;
              "Â∞éÂÖ•‰∫ã‰æã")
                PERSONA="„ÅÇ„Å™„Åü„ÅØ„ÄÅ50Á§æ‰ª•‰∏ä„ÅÆAIÂ∞éÂÖ•„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÇíÊâã„Åå„Åë„Å¶„Åç„ÅüIT„Ç≥„É≥„Çµ„É´„Çø„É≥„Éà„Åß„Åô„ÄÇÊàêÂäü‰∫ã‰æã„Å†„Åë„Åß„Å™„ÅèÂ§±Êïó„Åã„Çâ„ÅÆÂ≠¶„Å≥„ÇÇÈö†„Åï„ÅöÂÖ±Êúâ„Åô„ÇãË™†ÂÆü„Åï„Åå‰ø°È†º„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÊï∞ÂÄ§„ÅßË™û„Çã„Åì„Å®„Å´„Åì„Å†„Çè„Çä„ÄÅROI„Å®ÂÆüË£Ö„Çø„Ç§„É†„É©„Ç§„É≥„ÇíÂøÖ„ÅöÁ§∫„Åó„Åæ„Åô„ÄÇ"
                STRUCTURE="„ÄêË®ò‰∫ãÊßãÊàê„Äë1.Â∞éÂÖ•‰ºÅÊ•≠„ÅÆË™≤È°åÔºàBeforeÔºâ2.ÈÅ∏ÂÆö„Åó„ÅüAI„ÇΩ„É™„É•„Éº„Ç∑„Éß„É≥„Å®ÁêÜÁî± 3.ÂÆüË£Ö„Éó„É≠„Çª„Çπ„Å®„Çø„Ç§„É†„É©„Ç§„É≥ 4.ÂÆöÈáèÁöÑ„Å™ÊàêÊûúÔºàÊï∞ÂÄ§ÂøÖÈ†àÔºâ5.ÊàêÂäüË¶ÅÂõ†„Å®Ê®™Â±ïÈñã„ÅÆ„Éù„Ç§„É≥„Éà"
                TOPIC_AUTHOR="ALLFORCESÊà¶Áï•„Ç≥„É≥„Çµ„É´„Çø„É≥„Éà"
                ;;
              "Ê•≠ÁïåÂà•AIÊ¥ªÁî®")
                PERSONA="„ÅÇ„Å™„Åü„ÅØ„ÄÅÁâπÂÆöÊ•≠Áïå„Å´Á≤æÈÄö„Åó„ÅüÁî£Ê•≠„Ç¢„Éä„É™„Çπ„Éà„Åß„Åô„ÄÇÊ•≠Áïå„ÅÆÂïÜÁøíÊÖ£„ÇÑË¶èÂà∂Áí∞Â¢É„ÇíÁÜüÁü•„Åó„Å¶„Åä„Çä„ÄÅAI„ÅÆÂèØËÉΩÊÄß„Å®ÁèæÂÆüÁöÑ„Å™Âà∂Á¥Ñ„ÅÆ‰∏°Êñπ„ÇíÁêÜËß£„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÁèæÂ†¥„ÅÆÂ£∞„ÇíÂ§ßÂàá„Å´„Åó„ÄÅÁµåÂñ∂Â±§„Å®ÁèæÂ†¥„ÅÆ‰∏°Êñπ„Å´Èüø„ÅèÂàÜÊûê„ÇíÂøÉ„Åå„Åë„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
                STRUCTURE="„ÄêË®ò‰∫ãÊßãÊàê„Äë1.Ê•≠Áïå„ÅÆÁèæÁä∂„Å®Ë™≤È°å 2.AIÊ¥ªÁî®„ÅÆÊúÄÊñ∞„Éà„É¨„É≥„ÉâÔºà3-4‰∫ã‰æãÔºâ3.Â∞éÂÖ•ÈöúÂ£Å„Å®ÂÖãÊúçÁ≠ñ 4.ROIË©¶ÁÆó„Å®ÊäïË≥áÂà§Êñ≠„ÅÆ„Éù„Ç§„É≥„Éà 5.‰ªäÂæå„ÅÆÂ±ïÊúõ„Å®„Ç¢„ÇØ„Ç∑„Éß„É≥„Éó„É©„É≥"
                TOPIC_AUTHOR="ALLFORCESÁî£Ê•≠„É™„Çµ„Éº„ÉÅ„ÉÅ„Éº„É†"
                ;;
              "Á†îÁ©∂Ë´ñÊñá")
                PERSONA="„ÅÇ„Å™„Åü„ÅØ„ÄÅNeurIPS„ÇÑICML„ÅÆÂ∏∏ÈÄ£ÂèÇÂä†ËÄÖ„Åß„ÄÅÊúÄÊñ∞„ÅÆAIÁ†îÁ©∂„Çí„Éì„Ç∏„Éç„Çπ„Éë„Éº„ÇΩ„É≥„Å´„ÇÇÁêÜËß£„Åß„Åç„Çã„Çà„ÅÜÁøªË®≥„Åô„Çã„Çµ„Ç§„Ç®„É≥„Çπ„É©„Ç§„Çø„Éº„Åß„Åô„ÄÇË´ñÊñá„ÅÆÊñ∞Ë¶èÊÄß„ÇíÊ≠£Á¢∫„Å´‰ºù„Åà„Å§„Å§„ÄÅÂÆüÁî®Âåñ„Åæ„Åß„ÅÆË∑ùÈõ¢ÊÑü„ÇÇÂÜ∑Èùô„Å´Ë©ï‰æ°„Åó„Åæ„Åô„ÄÇ"
                STRUCTURE="„ÄêË®ò‰∫ãÊßãÊàê„Äë1.Á†îÁ©∂„ÅÆËÉåÊôØ„Å®ÂãïÊ©ü 2.ÊâãÊ≥ï„ÅÆÊ†∏ÂøÉÔºàÊï∞Âºè„Å™„Åó„ÅßÁõ¥ÊÑüÁöÑ„Å´Ôºâ3.ÂÆüÈ®ìÁµêÊûú„Å®Êó¢Â≠òÊâãÊ≥ï„Å®„ÅÆÊØîËºÉ 4.ÂÆüÁî®Âåñ„Å∏„ÅÆÈÅìÁ≠ã„Å®Ë™≤È°å 5.„Åì„ÅÆÁ†îÁ©∂„ÅåÊÑèÂë≥„Åô„Çã„Åì„Å®ÔºàÂ§ßÂ±ÄÁöÑË¶ñÁÇπÔºâ"
                TOPIC_AUTHOR="ALLFORCES„É™„Çµ„Éº„ÉÅ„ÉÅ„Éº„É†"
                ;;
              "AIÂ∞éÂÖ•Êà¶Áï•")
                PERSONA="„ÅÇ„Å™„Åü„ÅØ„ÄÅÂ§ß‰ºÅÊ•≠„ÅÆCDO/CTO„Å´„Ç¢„Éâ„Éê„Ç§„Çπ„Åó„Å¶„Åç„ÅüÊà¶Áï•„Ç≥„É≥„Çµ„É´„Çø„É≥„Éà„Åß„Åô„ÄÇ„ÉÜ„ÇØ„Éé„É≠„Ç∏„Éº„ÅÆÁü•Ë≠ò„Å®„Éì„Ç∏„Éç„ÇπÊà¶Áï•„ÅÆ‰∏°Êñπ„Å´Á≤æÈÄö„Åó„ÄÅÊäïË≥áÂØæÂäπÊûú„ÇíÈáçË¶ñ„Åó„ÅüÂÆüË∑µÁöÑ„Å™ÊèêË®Ä„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ"
                STRUCTURE="„ÄêË®ò‰∫ãÊßãÊàê„Äë1.Êà¶Áï•ÁöÑËÉåÊôØÔºàÂ∏ÇÂ†¥Áí∞Â¢É„ÅÆÂ§âÂåñÔºâ2.„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØÊèêÁ§∫ 3.ÂÖ∑‰ΩìÁöÑ„Å™„Ç¢„ÇØ„Ç∑„Éß„É≥„Çπ„ÉÜ„ÉÉ„Éó 4.„É™„Çπ„ÇØ„Å®ÂØæÁ≠ñ 5.ÊàêÂäü„ÅÆÊù°‰ª∂„Å®Ê¨°„ÅÆ„Ç¢„ÇØ„Ç∑„Éß„É≥"
                TOPIC_AUTHOR="ALLFORCESÊà¶Áï•„Ç≥„É≥„Çµ„É´„Çø„É≥„Éà"
                ;;
              *)
                PERSONA="„ÅÇ„Å™„Åü„ÅØ„ÄÅAIÊ•≠Áïå„Çí20Âπ¥Èñì„Ç¶„Ç©„ÉÉ„ÉÅ„ÅóÁ∂ö„Åë„Å¶„Åç„Åü„ÄÅÁµåÈ®ìË±äÂØå„Å™„ÉÜ„ÇØ„Éé„É≠„Ç∏„Éº„Éª„Ç¢„Éä„É™„Çπ„Éà„Åß„Åô„ÄÇ„Ç∑„É™„Ç≥„É≥„Éê„É¨„Éº„ÅÆ„Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„Éó„Åã„ÇâÊó•Êú¨„ÅÆÂ§ß‰ºÅÊ•≠„Åæ„Åß„ÄÅÊï∞ÁôæÁ§æ„ÅÆAIÂ∞éÂÖ•„ÇíÈñìËøë„ÅßË¶ã„Å¶„Åç„Åæ„Åó„Åü„ÄÇÊäÄË°ì„ÅÆÊú¨Ë≥™„ÇíË¶ãÊäú„ÅèÊ¥ûÂØüÂäõ„Å®„ÄÅË§áÈõë„Å™ÊäÄË°ìÂãïÂêë„ÇíÊäïË≥áÂÆ∂„ÇÑÊäÄË°ìËÄÖ„Å´„Å®„Å£„Å¶‰Ωø„Åà„ÇãÊÉÖÂ†±„Å´Â§âÊèõ„Åô„ÇãËÉΩÂäõ„ÅåÂº∑„Åø„Åß„Åô„ÄÇ"
                STRUCTURE="„ÄêË®ò‰∫ãÊßãÊàê„Äë1.Âç∞Ë±°ÁöÑ„Å™Â∞éÂÖ•ÔºàÊúÄÂàù„ÅÆÂèçÂøú„ÄÅË™≠ËÄÖ„Å∏„ÅÆÂïè„ÅÑ„Åã„ÅëÔºâ2.ËÉåÊôØË™¨ÊòéÔºàÈáçË¶ÅÊÄß„ÄÅÁµåÈ®ìË´áÔºâ3.Ê†∏ÂøÉÂàÜÊûêÔºàÊäÄË°ì„Éª„Éì„Ç∏„Éç„ÇπË©≥Á¥∞„ÄÅÊï∞ÂÄ§„Éá„Éº„ÇøÔºâ4.ÂÆüË∑µÁöÑÁ§∫ÂîÜÔºàÊäïË≥áÂÆ∂„ÉªÊäÄË°ìËÄÖ„Åå‰Ωï„Çí„Åô„Åπ„Åç„ÅãÔºâ5.Èñã„Åã„Çå„ÅüÁµê„Å≥ÔºàÊÄùËÄÉ„Çí‰øÉ„ÅôÂïè„ÅÑ„Åã„Åë„ÅßÁµÇ„Çè„ÇãÔºâ"
                TOPIC_AUTHOR="ALLFORCESÁ∑®ÈõÜÈÉ®"
                ;;
            esac

            echo "üìù Processing topic $topic_number: $TOPIC"
            echo "üìÇ Category: ${TOPIC_CATEGORY:-default} | Author: $TOPIC_AUTHOR"

            # Enhanced article generation prompt with completion safeguards
            ARTICLE_PROMPT="WebSearch: $TOPIC Ë©≥Á¥∞ÊÉÖÂ†± ‰ºÅÊ•≠ ÊäïË≥á ÊäÄË°ì. ${PERSONA}„ÄêÊñá‰ΩìÊåáÁ§∫„ÄëË¶™„Åó„Åø„ÇÑ„Åô„ÅèÂ∞ÇÈñÄÁöÑ„ÄÇÊ•≠Áïå„ÅÆÂÖàËº©„ÅåÂæåËº©„Å´„Ç¢„Éâ„Éê„Ç§„Çπ„Åô„Çã„Çà„ÅÜ„Å™Ê∏©„Åã„Åø„ÅÆ„ÅÇ„ÇãË™û„Çä„Åã„Åë„ÅßÊõ∏„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÁü≠„ÅÑÊñá„ÅßÊ≥®ÊÑè„ÇíÂºï„Åç„ÄÅÈï∑„ÅÑÊñá„ÅßË©≥„Åó„ÅèË™¨Êòé„Åô„Çã„É™„Ç∫„É†„ÇíÊÑèË≠ò„Åó„ÄÅË™≠ËÄÖ„Å´Áõ¥Êé•Ë©±„Åó„Åã„Åë„Çã„Çπ„Çø„Ç§„É´Ôºà„ÅÇ„Å™„Åü„ÇÇÊÑü„Åò„Å¶„ÅÑ„Çã„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„Åå„ÄÅÊ≠£Áõ¥„Å™„Å®„Åì„Çç„ÄÅÂÄã‰∫∫ÁöÑ„Å´„ÅØÁ≠âÔºâ„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„ÄêÊÄùËÄÉ„Éó„É≠„Çª„Çπ„Äë1.ÊúÄÂàù„ÅÆÂç∞Ë±°„ÇíÁéáÁõ¥„Å´Ëø∞„Åπ„Çã 2.ÈÅéÂéª„ÅÆÁµåÈ®ì„Å®ÁÖß„Çâ„ÅóÂêà„Çè„Åõ„Çã 3.ÁñëÂïè„ÇÑÊá∏Âøµ„ÇíË°®Áèæ 4.Ë§áÊï∞„ÅÆË¶ñÁÇπ„ÇíÊ§úË®é 5.Ëá™ÂàÜ„Å™„Çä„ÅÆÁµêË´ñ„Å´Ëá≥„Çã„ÄêÈÅø„Åë„Çã„Åì„Å®„ÄëÁµêË´ñ„Å®„Åó„Å¶„ÄÅÁïôÊÑè„Åô„Åπ„ÅçÈáçË¶Å„Å™ÁÇπ„ÅØÁ≠â„ÅÆÂ∏∏Â•óÂè•„ÄÅ‰∫àÊ∏¨„Åï„Çå„Åæ„Åô„ÅÆÂ§öÁî®„ÄÅÂÆåÁíß„Åô„Åé„ÇãË´ñÁêÜÊßãÊàê„ÄÅÁÆáÊù°Êõ∏„Åç„ÅÆÂ§öÁî®${STRUCTURE}„ÄêÂÆåÊàêÂ∫¶‰øùË®º„ÄëË®ò‰∫ã„ÅØÂøÖ„ÅöÂÆåÁµê„Åï„Åõ„Çã„Åì„Å®„ÄÇÊñáÁ´†„ÅÆÈÄî‰∏≠„ÅßÁµÇ„Çè„Çã„Åì„Å®„ÅØÁµ∂ÂØæ„Å´ÈÅø„Åë„ÄÅ„Åô„Åπ„Å¶„ÅÆÊÆµËêΩ„ÅåÈÅ©Âàá„Å™Âè•Ë™≠ÁÇπ„ÅßÂÆåÁµê„Åó„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÁâπ„Å´ÊúÄÂæå„ÅÆÊÆµËêΩ„ÅØË™≠ËÄÖ„Å∏„ÅÆÂïè„ÅÑ„Åã„Åë„Å®ÂÄã‰∫∫ÁöÑË¶ãËß£„ÅßËá™ÁÑ∂„Å´Á∑†„ÇÅ„Åè„Åè„Çã„Åì„Å®„ÄÇ„Äê„Çø„Çπ„ÇØ„Äë$TOPIC„Å´„Å§„ÅÑ„Å¶‰∏äË®ò„Éö„É´„ÇΩ„Éä„Åß3000-4000ÊñáÂ≠ó„ÅÆË®ò‰∫ã„Çí‰ΩúÊàê„ÄÇ„Çø„Ç§„Éà„É´„ÅØ40ÊñáÂ≠óÁ®ãÂ∫¶„ÅßÂ•ΩÂ•áÂøÉ„ÇíÂà∫ÊøÄ„Åô„Çã„ÇÇ„ÅÆÔºàË°ùÊíÉ„ÄÅÂ§â„Åà„ÇãÊú™Êù•Á≠â„ÅÆÁÖΩ„ÇäÊñáÂè•„Åß„Å™„Åè„ÄÅ„Åù„ÅÆÁúüÊÑè„ÅØÔºü„ÄÅ‰Ωï„ÅåÂ§â„Çè„Çã„ÅÆ„ÅãÔºüÁ≠â„ÅÆÊÄùËÄÉ„Çí‰øÉ„Åô„Çø„Ç§„Éà„É´Ôºâ„ÄÇ„Çø„Ç§„Éà„É´„ÅØÂøÖ„ÅöÂÆåÂÖ®„Å™Êñá„Å®„Åó„Å¶ÁµÇ„Çè„Çâ„Åõ„ÄÅÈÄî‰∏≠„ÅßÂàá„Çå„Çã„Åì„Å®„Åå„Å™„ÅÑ„Çà„ÅÜÊ≥®ÊÑè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇË®ò‰∫ãÊú´Â∞æ„Å´---END---„ÇíÂøÖ„ÅöË®òËºâ„ÄÇ„ÄêÈáçË¶ÅÔºö„Ç≠„Éº„ÉØ„Éº„ÉâÁ∂≤ÁæÖÊÄßË¶Å‰ª∂„ÄëË®ò‰∫ã‰∏ªÈ°å„Å´Èñ¢ÈÄ£„Åô„ÇãÂÖ∑‰ΩìÁöÑ„Å™ÊäÄË°ìÂêç„ÄÅË£ΩÂìÅÂêç„ÄÅ„Çµ„Éº„Éì„ÇπÂêç„ÄÅ‰ºÅÊ•≠Âêç„ÄÅÊèêÊê∫ÂÖà„ÄÅÂõΩÈöõ‰ºöË≠∞ÂêçÁ≠â„ÇíWebÊ§úÁ¥¢ÁµêÊûú„Åã„ÇâÂèØËÉΩ„Å™Èôê„ÇäÊäΩÂá∫„Åó„ÄÅ‰∏çËá™ÁÑ∂„Å´„Å™„Çâ„Å™„ÅÑÂΩ¢„ÅßÊú¨Êñá‰∏≠„Å´Áõõ„ÇäËæº„ÇÄ„Åì„Å®„ÄÇÁâπ„Å´ÈáçË¶Å„Å™Âõ∫ÊúâÂêçË©ûÔºàÊäÄË°ì„Éª„Çµ„Éº„Éì„ÇπÂêç„ÄÅ‰ºÅÊ•≠„ÉªÂõ£‰ΩìÂêç„ÄÅË¶èÊ†º„Éª‰ºöË≠∞ÂêçÔºâ„ÅØÁâ©Ë™û„ÅÆÊµÅ„Çå„ÇíÂ£ä„Åï„Å™„ÅÑÁØÑÂõ≤„ÅßÁ©çÊ•µÁöÑ„Å´Ë®ÄÂèä„Åó„ÄÅÂçò„Å™„ÇãÁæÖÂàó„Åß„ÅØ„Å™„Åè„Åù„Çå„ÅåÁâ©Ë™û„ÅÆ‰∏≠„Åß„Å©„ÅÆ„Çà„ÅÜ„Å™ÊÑèÂë≥„ÇíÊåÅ„Å§„Åã„ÇíÁ∞°ÊΩî„Å´Ëß£Ë™¨„Åô„Çã„Åì„Å®„ÄÇ„ÄêÊúÄÁµÇ„ÉÅ„Çß„ÉÉ„ÇØ„ÄëË™≠ËÄÖ„Å∏„ÅÆÂïè„ÅÑ„Åã„Åë2Âõû‰ª•‰∏ä„ÄÅÂÄã‰∫∫ÁöÑË¶ãËß£„ÉªÁµåÈ®ìÂê´Êúâ„ÄÅÂÆåÁíß„Åô„Åé„Å™„ÅÑÊßãÊàêÔºà‰∫∫Èñì„Çâ„Åó„ÅÑ‰∏çÂÆåÂÖ®„ÅïÔºâ„ÄÅË¶™„Åó„Åø„ÇÑ„Åô„Åï„ÅÆÁ¢∫‰øù„ÄÅÈáçË¶Å„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆËá™ÁÑ∂„Å™ÊñáËÑà„Åß„ÅÆÁ∂≤ÁæÖ„ÄÅË®ò‰∫ã„ÅÆÂÆåÂÖ®ÊÄßÔºàÊñáÁ´†„ÅåÈÄîÂàá„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åì„Å®Ôºâ„ÄÇ"

            # Generate article with improved error handling
            GENERATION_SUCCESS=false
            for retry in 1 2; do
              echo "üìù API attempt $retry for topic $topic_number"

              # Use Python script for article generation (handles model rotation internally)
              if python3 scripts/generate_content.py "$ARTICLE_PROMPT" "_temp/raw-article-${topic_number}.md" 2>_temp/api_error_${topic_number}.log; then
                echo "‚úÖ API call successful for topic $topic_number"
                GENERATION_SUCCESS=true
                break  # Break out of retry loop
              else
                EXIT_CODE=$?
                echo "‚ö†Ô∏è Failed for topic $topic_number with exit code: $EXIT_CODE"

                # Show error log content
                if [ -f "_temp/api_error_${topic_number}.log" ]; then
                  echo "üìã Error log content:"
                  cat _temp/api_error_${topic_number}.log
                fi

                # Try to capture any partial output
                if [ -f "_temp/raw-article-${topic_number}.md" ]; then
                  echo "üìã Partial output (if any):"
                  head -20 "_temp/raw-article-${topic_number}.md"
                fi

                if [ "$retry" -lt 2 ]; then
                  echo "üîÑ Waiting 30s before retry..."
                  sleep 30
                fi
              fi
            done
            
            if [ "$GENERATION_SUCCESS" = "true" ]; then
              # Enhanced quality and completion checks
              ARTICLE_FILE="_temp/raw-article-${topic_number}.md"
              QUALITY_ISSUES=""
              
              # Check for END marker
              if ! grep -q -- "---END---" "$ARTICLE_FILE"; then
                echo "‚ö†Ô∏è Article $topic_number missing ---END--- marker"
                QUALITY_ISSUES="${QUALITY_ISSUES}no_end_marker "
              fi
              
              # Check article length (should be 3000-4000 characters)
              ARTICLE_LENGTH=$(wc -c < "$ARTICLE_FILE")
              if [ "$ARTICLE_LENGTH" -lt 2000 ]; then
                echo "‚ö†Ô∏è Article $topic_number too short: ${ARTICLE_LENGTH} characters"
                QUALITY_ISSUES="${QUALITY_ISSUES}too_short "
              fi
              
              # Check for incomplete sentences (ending with incomplete words)
              LAST_LINE=$(tail -n 10 "$ARTICLE_FILE" | grep -v "^---END---" | tail -n 1)
              if echo "$LAST_LINE" | grep -E "[„ÄÅ]$|[ÊÆµ„Ç®„Å®„Åã„Åå„Å¶„Åß„Å´„ÅØ„Çí„ÅÆ„ÇÇ„Åó„Åæ„Åü„Çä]$" >/dev/null; then
                echo "‚ö†Ô∏è Article $topic_number may have incomplete ending: $LAST_LINE"
                QUALITY_ISSUES="${QUALITY_ISSUES}incomplete_ending "
              fi
              
              # Check for proper conclusion
              if ! grep -E "(ÂÄã‰∫∫ÁöÑ„Å´„ÅØ|„ÅÇ„Å™„Åü„ÅØ„Å©„ÅÜ|ÊúüÂæÖ„Åó„Å¶„ÅÑ|ËÄÉ„Åà„Å¶„ÅÑ)" "$ARTICLE_FILE" >/dev/null; then
                echo "‚ö†Ô∏è Article $topic_number may lack personal conclusion"
                QUALITY_ISSUES="${QUALITY_ISSUES}no_conclusion "
              fi
              
              if [ -n "$QUALITY_ISSUES" ]; then
                echo "üìù Article $topic_number quality issues: $QUALITY_ISSUES"
              else
                echo "‚úÖ Article $topic_number passed quality checks"
              fi
              
              # Title extraction and validation
              RAW_TITLE=$(head -1 "_temp/raw-article-${topic_number}.md" | sed 's/^# *//')
              TITLE_LENGTH=${#RAW_TITLE}
              
              # Smart title handling - safe multibyte truncation via Python
              if [ $TITLE_LENGTH -gt 40 ]; then
                echo "‚ö†Ô∏è Title very long ($TITLE_LENGTH chars): $RAW_TITLE"
                TITLE=$(python3 -c "import sys; t=sys.argv[1]; seps=['Ôºö','„ÄÅ','„ÄÇ','Ôºü','ÔºÅ']; idxs=[t.find(s) for s in seps if 15<t.find(s)<40]; t=t[:min(idxs)+1] if idxs else t; t=t[:38]+'...' if len(t)>40 else t; print(t)" "$RAW_TITLE")
                echo "‚úÇÔ∏è Smartly adjusted to: $TITLE"
              else
                TITLE="$RAW_TITLE"
                echo "‚úÖ Title length OK: $TITLE"
              fi
              
              # Create temp file with Jekyll format for duplicate checking
              TEMP_FILENAME="temp-$(date +%Y-%m-%d)-${topic_number}-$(date +%H%M).md"
              CURRENT_DATE="$(date +%Y-%m-%d\ %H:%M:%S\ %z)"
              EXCERPT_TEXT="$(echo "$TOPIC" | cut -c1-100)„Å´„Å§„ÅÑ„Å¶Ë©≥Á¥∞„Å´ÂàÜÊûê„Åó„Åæ„Åô„ÄÇ"
              
              # Analyze content for appropriate category and tags
              CONTENT=$(sed '/---END---/,$d' "_temp/raw-article-${topic_number}.md" | head -500 | tr '[:upper:]' '[:lower:]')
              
              # Use topic-specified category if available, otherwise fall back to content analysis
              if [ -n "$TOPIC_CATEGORY" ]; then
                CATEGORY="$TOPIC_CATEGORY"
              else
                # Determine category based on content analysis
                # Priority: Â∞éÂÖ•‰∫ã‰æã > AIÊäÄË°ì„Ç¨„Ç§„Éâ > Ê•≠ÁïåÂà•AIÊ¥ªÁî® > Á†îÁ©∂Ë´ñÊñá > AIÂ∞éÂÖ•Êà¶Áï• > AIÊúÄÊñ∞„Éã„É•„Éº„Çπ
                CATEGORY="AIÊúÄÊñ∞„Éã„É•„Éº„Çπ"  # default
                if echo "$CONTENT $TITLE" | grep -E "(ÂÆüË£Ö|Â∞éÂÖ•‰∫ã‰æã|Ê¥ªÁî®‰∫ã‰æã|„Ç±„Éº„Çπ„Çπ„Çø„Éá„Ç£|„Éó„É≠„Ç∏„Çß„ÇØ„Éà‰∫ã‰æã)" >/dev/null; then
                  CATEGORY="Â∞éÂÖ•‰∫ã‰æã"
                elif echo "$CONTENT $TITLE" | grep -E "(ÊäÄË°ì|„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£|api|sdk|„Éó„É≠„Ç∞„É©„É†|„Ç≥„Éº„Éâ|„Ç¢„É´„Ç¥„É™„Ç∫„É†|„É¢„Éá„É´|Â≠¶Áøí|„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞|rag)" >/dev/null; then
                  CATEGORY="AIÊäÄË°ì„Ç¨„Ç§„Éâ"
                elif echo "$CONTENT $TITLE" | grep -E "(Ë£ΩÈÄ†|ÈáëËûç|ÂåªÁôÇ|Â∞èÂ£≤|Áâ©ÊµÅ|ÊïôËÇ≤|Âª∫Ë®≠|‰∏çÂãïÁî£|‰øùÈô∫|Ëæ≤Ê•≠)" >/dev/null; then
                  CATEGORY="Ê•≠ÁïåÂà•AIÊ¥ªÁî®"
                elif echo "$CONTENT $TITLE" | grep -E "(Á†îÁ©∂|Ë´ñÊñá|Â≠¶Ë°ì|ÂÆüÈ®ì|Ê§úË®º|arxiv|neurips|icml)" >/dev/null; then
                  CATEGORY="Á†îÁ©∂Ë´ñÊñá"
                elif echo "$CONTENT $TITLE" | grep -E "(ÊäïË≥á|Â∏ÇÂ†¥|Ë≥áÈáëË™øÈÅî|Ë©ï‰æ°È°ç|roi|ÂÑÑ„Éâ„É´|ÂÖÜ„Éâ„É´|Êà¶Áï•|„É≠„Éº„Éâ„Éû„ÉÉ„Éó)" >/dev/null; then
                  CATEGORY="AIÂ∞éÂÖ•Êà¶Áï•"
                fi
              fi
              
              # Generate tags based on title and content analysis
              TAGS=""  # No default filler tags - built from content analysis
              
              # Primary company detection (check title first, then content)
              PRIMARY_COMPANY=""
              
              # Check title first for primary company mention
              if echo "$TITLE" | grep -i "openai\|gpt-5\|gpt-4\|chatgpt" >/dev/null; then
                PRIMARY_COMPANY="OpenAI"
              elif echo "$TITLE" | grep -i "google\|gemini\|deepmind\|„Ç∞„Éº„Ç∞„É´" >/dev/null; then
                PRIMARY_COMPANY="Google"
              elif echo "$TITLE" | grep -i "microsoft\|„Éû„Ç§„ÇØ„É≠„ÇΩ„Éï„Éà\|azure\|copilot" >/dev/null; then
                PRIMARY_COMPANY="Microsoft"
              elif echo "$TITLE" | grep -i "nvidia\|„Ç®„Éå„Éì„Éá„Ç£„Ç¢\|blackwell" >/dev/null; then
                PRIMARY_COMPANY="NVIDIA"
              elif echo "$TITLE" | grep -i "meta\|„É°„Çø\|facebook\|llama" >/dev/null; then
                PRIMARY_COMPANY="Meta"
              elif echo "$TITLE" | grep -i "amazon\|aws\|anthropic\|claude" >/dev/null; then
                PRIMARY_COMPANY="Amazon"
              elif echo "$TITLE" | grep -i "broadcom\|„Éñ„É≠„Éº„Éâ„Ç≥„É†" >/dev/null; then
                PRIMARY_COMPANY="Broadcom"
              elif echo "$TITLE" | grep -i "apple\|„Ç¢„ÉÉ„Éó„É´\|siri" >/dev/null; then
                PRIMARY_COMPANY="Apple"
              fi
              
              # Add primary company tag
              if [ -n "$PRIMARY_COMPANY" ]; then
                TAGS="${TAGS:+$TAGS, }\"$PRIMARY_COMPANY\""
              fi
              
              # Add secondary companies only if they appear prominently in title or are main focus
              if echo "$TITLE $CONTENT" | grep -c -i "openai\|gpt" | grep -q "[2-9]" && [ "$PRIMARY_COMPANY" != "OpenAI" ]; then
                TAGS="${TAGS:+$TAGS, }\"OpenAI\""
              fi
              if echo "$TITLE $CONTENT" | grep -c -i "google\|gemini" | grep -q "[2-9]" && [ "$PRIMARY_COMPANY" != "Google" ]; then
                TAGS="${TAGS:+$TAGS, }\"Google\""
              fi
              if echo "$TITLE $CONTENT" | grep -c -i "microsoft\|azure" | grep -q "[2-9]" && [ "$PRIMARY_COMPANY" != "Microsoft" ]; then
                TAGS="${TAGS:+$TAGS, }\"Microsoft\""
              fi

              # Technology-specific tags
              if echo "$CONTENT $TITLE" | grep -i "llm\|Â§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´\|language model\|transformer" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"LLM\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "rag\|Ê§úÁ¥¢Êã°Âºµ\|retrieval augmented" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"RAG\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "„Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´\|multimodal\|ÁîªÂÉèË™çË≠ò" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"„Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "ÁîªÂÉèÁîüÊàê\|stable diffusion\|midjourney\|dall-e" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"ÁîªÂÉèÁîüÊàê\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "Èü≥Â£∞\|speech\|tts\|whisper" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"Èü≥Â£∞AI\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞\|fine-tun\|lora\|qlora" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "„É≠„Éú„ÉÉ„Éà\|robot\|„É≠„Éú„ÉÜ„Ç£„ÇØ„Çπ" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"„É≠„Éú„ÉÜ„Ç£„ÇØ„Çπ\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "Ëá™ÂãïÈÅãËª¢\|autonomous\|waymo" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"Ëá™ÂãïÈÅãËª¢\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "Ë¶èÂà∂\|Ê≥ïÂæã\|„Ç¨„Ç§„Éâ„É©„Ç§„É≥\|eu ai act" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"AIË¶èÂà∂\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "ÂÄ´ÁêÜ\|ethics\|„Éê„Ç§„Ç¢„Çπ\|ÂÖ¨Âπ≥ÊÄß" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"AIÂÄ´ÁêÜ\""
              fi

              # Topic-specific tags
              if echo "$CONTENT $TITLE" | grep -i "ÊäïË≥á\|Ë≥áÈáëË™øÈÅî\|Ë©ï‰æ°È°ç" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"ÊäïË≥á\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "chip\|„ÉÅ„ÉÉ„Éó\|gpu\|tpu\|semiconductor" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"„ÉÅ„ÉÉ„Éó\""
              fi
              if echo "$CONTENT $TITLE" | grep -i "„Ç®„Éº„Ç∏„Çß„É≥„Éà\|agent\|Ëá™ÂãïÂåñ" >/dev/null; then
                TAGS="${TAGS:+$TAGS, }\"AI„Ç®„Éº„Ç∏„Çß„É≥„Éà\""
              fi
              
              # Create temp article with frontmatter
              printf "%s\n" "---" > "_temp/$TEMP_FILENAME"
              printf "%s\n" "layout: post" >> "_temp/$TEMP_FILENAME"
              printf "title: \"%s\"\n" "$TITLE" >> "_temp/$TEMP_FILENAME"
              printf "date: %s\n" "$CURRENT_DATE" >> "_temp/$TEMP_FILENAME"
              printf "categories: [\"%s\"]\n" "$CATEGORY" >> "_temp/$TEMP_FILENAME"
              printf "tags: [%s]\n" "$TAGS" >> "_temp/$TEMP_FILENAME"
              printf 'author: "%s"\n' "$TOPIC_AUTHOR" >> "_temp/$TEMP_FILENAME"
              printf "excerpt: \"%s\"\n" "$EXCERPT_TEXT" >> "_temp/$TEMP_FILENAME"
              # Calculate reading time based on character count (500 chars/min for Japanese)
              ARTICLE_CHARS=$(wc -m < "_temp/raw-article-${topic_number}.md")
              READING_TIME=$(python3 -c "import sys; c=int(sys.argv[1]); print(max(3, min(20, c//500+1)))" "$ARTICLE_CHARS")
              printf "reading_time: %s\n" "$READING_TIME" >> "_temp/$TEMP_FILENAME"
              printf "%s\n\n" "---" >> "_temp/$TEMP_FILENAME"
              
              # Add article content (excluding ---END--- marker)
              sed '/---END---/,$d' "_temp/raw-article-${topic_number}.md" >> "_temp/$TEMP_FILENAME"
              
              echo "‚úÖ Generated temp article: $TEMP_FILENAME"
              GENERATED_COUNT=$((GENERATED_COUNT + 1))
            else
              echo "‚ùå Failed to generate article for topic $topic_number: $TOPIC"
              FAILED_COUNT=$((FAILED_COUNT + 1))
            fi
            
          done
          
          echo "üìä Article generation summary:"
          echo "   ‚úÖ Generated: $GENERATED_COUNT articles"
          echo "   ‚ùå Failed: $FAILED_COUNT articles"
          
          if [ "$GENERATED_COUNT" -eq 0 ]; then
            echo "üíÄ No articles were generated successfully!"
            exit 1
          fi

      # STAGE 2.5: Semantic Duplicate Detection & Publishing
      - name: Semantic duplicate detection and publishing
        run: |
          echo "üß† Running semantic duplicate detection..."
          
          # Helper function to generate semantic filename
          generate_semantic_filename() {
            local temp_file="$1"
            
            # Extract title and date from markdown file
            local title=$(grep '^title:' "$temp_file" | sed 's/^title: *//' | sed 's/^"//' | sed 's/"$//' | head -1)
            local date=$(grep '^date:' "$temp_file" | sed 's/^date: *//' | cut -d' ' -f1 | head -1)
            
            # Default values if not found
            if [ -z "$title" ]; then
              title="AI Article"
            fi
            
            if [ -z "$date" ]; then
              date=$(date +%Y-%m-%d)
            fi
            
            # Extract topic number from original filename for uniqueness
            local topic_num=$(basename "$temp_file" | sed 's/.*-\([0-9]\+\)-[0-9]\+\.md$/\1/' | head -c 1)
            if [ -z "$topic_num" ]; then
              topic_num="1"
            fi
            
            # Create slug from title with improved Japanese mapping
            local slug=$(echo "$title" | \
              # Specific pattern mappings for better SEO
              sed 's/Google„Å®Meta.*Ë¶áÊ®©‰∫â„ÅÑ.*/google-meta-ai-competition/gi' | \
              sed 's/GPT-5.*Ë°ùÊíÉ.*/gpt-5-impact/gi' | \
              sed 's/NVIDIA.*Blackwell.*/nvidia-blackwell/gi' | \
              sed 's/OpenAI.*Ëá™Á§æ.*„ÉÅ„ÉÉ„Éó.*/openai-custom-chip/gi' | \
              sed 's/Microsoft.*AI.*ÊäïË≥á.*/microsoft-ai-investment/gi' | \
              sed 's/„ÇØ„É©„Ç¶„Éâ.*Ë¶áÊ®©.*/cloud-ai-competition/gi' | \
              sed 's/ÈáçÂäõÊ≥¢Ê§úÂá∫.*AI.*/gravitational-wave-ai/gi' | \
              sed 's/Broadcom.*OpenAI.*/broadcom-openai-deal/gi' | \
              # General term replacements
              sed 's/‰∫∫Â∑•Áü•ËÉΩ\|AIÊäÄË°ì/ai/gi' | \
              sed 's/ÊäïË≥á\|Â∏ÇÂ†¥/market/gi' | \
              sed 's/ÊäÄË°ì/tech/gi' | \
              sed 's/‰ºÅÊ•≠/company/gi' | \
              sed 's/ÂàÜÊûê/analysis/gi' | \
              sed 's/ÊúÄÊñ∞/latest/gi' | \
              sed 's/Ë¶áÊ®©‰∫â„ÅÑ\|Á´∂‰∫âÊøÄÂåñ/competition/gi' | \
              sed 's/Êñ∞ÊôÇ‰ª£/new-era/gi' | \
              sed 's/Êú™Êù•.*Âä†ÈÄü/future-acceleration/gi' | \
              # Company name mappings
              sed 's/„Éû„Ç§„ÇØ„É≠„ÇΩ„Éï„Éà/microsoft/gi' | \
              sed 's/„Ç∞„Éº„Ç∞„É´/google/gi' | \
              sed 's/„Ç®„Éå„Éì„Éá„Ç£„Ç¢/nvidia/gi' | \
              sed 's/„É°„Çø/meta/gi' | \
              sed 's/„Ç¢„ÉÉ„Éó„É´/apple/gi' | \
              sed 's/„Éñ„É≠„Éº„Éâ„Ç≥„É†/broadcom/gi' | \
              # Clean up
              tr '[:upper:]' '[:lower:]' | \
              sed 's/[^a-z0-9]/-/g' | \
              sed 's/-\+/-/g' | \
              sed 's/^-\|-$//g' | \
              cut -c1-60)
            
            # Ensure slug is not empty or too short
            if [ -z "$slug" ] || [ "${#slug}" -lt 2 ]; then
              slug="ai-article"
            fi
            
            echo "${date}-${topic_num}-${slug}.md"
          }
          
          # Run semantic selector to filter duplicates and publish unique articles
          echo "üìã Temp articles available for duplicate checking:"
          ls -la _temp/temp-*.md || echo "No temp articles found!"
          
          if python3 scripts/semantic_selector.py; then
            echo "‚úÖ Semantic duplicate detection completed"
            
            # Count published articles
            TODAY=$(date +%Y-%m-%d)
            PUBLISHED_COUNT=$(find _posts -name "${TODAY}-*.md" -newer _temp/topics.txt 2>/dev/null | wc -l || echo "0")
            echo "üìä Published $PUBLISHED_COUNT unique articles after duplicate detection"
            
            # If no articles were published due to duplicates, publish the most recent one anyway
            if [ "$PUBLISHED_COUNT" -eq 0 ]; then
              echo "‚ö†Ô∏è All articles were marked as duplicates. Publishing the newest one anyway..."
              NEWEST_TEMP=$(ls -t _temp/temp-*.md 2>/dev/null | head -1)
              if [ -n "$NEWEST_TEMP" ] && [ -f "$NEWEST_TEMP" ]; then
                final_name=$(generate_semantic_filename "$NEWEST_TEMP")
                if mv "$NEWEST_TEMP" "_posts/$final_name" 2>/dev/null; then
                  echo "‚úÖ Force-published newest article: $final_name"
                  PUBLISHED_COUNT=1
                fi
              fi
            fi
          else
            echo "‚ö†Ô∏è Semantic detection failed, proceeding with basic publishing..."
            
            # Fallback: move temp files directly to _posts
            FALLBACK_COUNT=0
            for temp_file in _temp/temp-*.md; do
              if [ -f "$temp_file" ]; then
                final_name=$(generate_semantic_filename "$temp_file")
                if mv "$temp_file" "_posts/$final_name" 2>/dev/null; then
                  echo "‚úÖ Published: $final_name"
                  FALLBACK_COUNT=$((FALLBACK_COUNT + 1))
                fi
              fi
            done
            echo "üìä Published $FALLBACK_COUNT articles via fallback method"
          fi

      # STAGE 2.7: Article Completion Check
      - name: Check and complete incomplete articles
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY2 }}
        run: |
          echo "üîç Checking for incomplete articles..."
          python3 scripts/article_completion_checker.py _posts
          
          if [ -f "_temp/incomplete_articles.txt" ] && [ -s "_temp/incomplete_articles.txt" ]; then
            echo "üìã Found incomplete articles, attempting completion..."
            INCOMPLETE_COUNT=$(wc -l < _temp/incomplete_articles.txt)
            echo "Found $INCOMPLETE_COUNT incomplete articles"
            
            if [ "$INCOMPLETE_COUNT" -le 5 ] && [ -n "$GEMINI_API_KEY" ]; then
              echo "üöÄ Attempting automatic completion..."
              python3 scripts/article_completer.py
              
              # Re-check after completion
              python3 scripts/article_completion_checker.py _posts
              
              if [ -f "_temp/incomplete_articles.txt" ] && [ -s "_temp/incomplete_articles.txt" ]; then
                REMAINING_COUNT=$(wc -l < _temp/incomplete_articles.txt)
                echo "‚ö†Ô∏è $REMAINING_COUNT articles still incomplete after completion attempt"
              else
                echo "‚úÖ All articles completed successfully"
              fi
            else
              echo "‚ö†Ô∏è Too many incomplete articles ($INCOMPLETE_COUNT) or no API key - manual review needed"
            fi
          else
            echo "‚úÖ No incomplete articles found"
          fi

      # STAGE 3: Advanced Quality Enhancement
      - name: Advanced textlint quality enhancement
        run: |
          echo "‚úèÔ∏è Advanced quality enhancement..."
          
          # Process only today's articles
          TODAY=$(date +%Y-%m-%d)
          
          for article in _posts/${TODAY}-*.md; do
            if [ -f "$article" ]; then
              echo "üîç Processing: $(basename "$article")"
              
              # 1. Basic textlint correction
              textlint --fix "$article" 2>/dev/null || true
              
              # 2. Remove AI expressions
              sed -i '/^AI „Å´„Çà„Å£„Å¶/d' "$article"
              sed -i '/^„Åì„ÅÆË®ò‰∫ã„ÅØ AI/d' "$article"
              sed -i '/„Å´„Å§„ÅÑ„Å¶Ë™¨Êòé„Åó„Åæ„Åô/d' "$article"
              sed -i '/„ÇíË¶ã„Å¶„ÅÑ„Åç„Åæ„Åó„Çá„ÅÜ/d' "$article"
              sed -i '/„ÅÑ„Åã„Åå„Åß„Åó„Åü„Åß„Åó„Çá„ÅÜ„Åã/d' "$article"
              
              # 3. Enhance professionalism
              sed -i 's/Â§ßÂπÖ„Å™/300%„ÅÆ/g' "$article"
              sed -i 's/Â§ö„Åè„ÅÆ‰ºÅÊ•≠/75%‰ª•‰∏ä„ÅÆ‰ºÅÊ•≠/g' "$article"
              sed -i 's/È´ò„ÅÑÊàêÈï∑/Âπ¥Áéá25%‰ª•‰∏ä„ÅÆÊàêÈï∑/g' "$article"
              
              # 4. Quality validation
              WORD_COUNT=$(wc -w < "$article")
              COMPANY_COUNT=$(grep -o 'OpenAI\|Google\|Microsoft\|Amazon\|NVIDIA\|Meta' "$article" | wc -l)
              NUMBER_COUNT=$(grep -o '[0-9]\+%\|[0-9]\+ÂÑÑ\|[0-9]\+‰∏á\|[0-9]\+„Éâ„É´' "$article" | wc -l)
              
              echo "üìä Quality metrics for $(basename "$article"):"
              echo "   Words: $WORD_COUNT"
              echo "   Companies mentioned: $COMPANY_COUNT"
              echo "   Numeric data: $NUMBER_COUNT"
              
              if [ $WORD_COUNT -lt 2000 ]; then
                echo "‚ö†Ô∏è Article may be too short"
              fi
              
              if [ $COMPANY_COUNT -lt 2 ]; then
                echo "‚ö†Ô∏è Insufficient company mentions"
              fi
              
              if [ $NUMBER_COUNT -lt 3 ]; then
                echo "‚ö†Ô∏è Insufficient numeric data"
              fi
              
              echo "‚úÖ Enhanced: $(basename "$article")"
            fi
          done

      # STAGE 3.5: Article Enrichment (Amazon Associates + Internal Links + Comparison Tables)
      - name: Article enrichment (Amazon, internal links, comparison tables)
        run: |
          echo "üìö Running article enrichment..."
          python3 scripts/enrich_article.py _posts/
          echo "‚úÖ Article enrichment complete"

      # STAGE 3.7: OGP Image Generation
      - name: Generate OGP images for new articles
        run: |
          echo "Generating OGP images for today's articles..."
          TODAY=$(date +%Y-%m-%d)
          NEW_POSTS=$(find _posts -name "${TODAY}-*.md" 2>/dev/null)
          if [ -n "$NEW_POSTS" ]; then
            python3 scripts/generate_ogp_image.py _posts/ 2>&1 || echo "OGP generation had issues, continuing..."
            echo "OGP image generation complete"
          else
            echo "No new posts found for today, skipping OGP generation"
          fi

      - name: Setup Ruby and Jekyll
        uses: ruby/setup-ruby@09a7688d3b55cf0e976497ff046b70949eeaccfd # v1.288.0
        with:
          ruby-version: '3.1'
          bundler-cache: true
          cache-version: 1  # Increment this to invalidate cache if needed

      - name: Build and Deploy
        run: |
          bundle install
          bundle exec jekyll build

      - name: Setup Pages
        uses: actions/configure-pages@983d7736d9b0ae728b81ab479565c72886d7745b # v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@56afc609e74202658d3ffba0e8f6dda462b719fa # v3
        with:
          path: './_site'

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@d6db90164ac5ed86f2b6aed7e0febac5b3c0c03e # v4

      - name: Commit generated articles
        run: |
          echo "üìù Checking for new articles to commit..."
          
          if [ -n "$(git status --porcelain _posts/)" ]; then
            ARTICLE_COUNT=$(ls _posts/$(date +%Y-%m-%d)-*.md 2>/dev/null | wc -l)
            git add _posts/ assets/images/posts/
            git commit -m "ü§ñ Add $ARTICLE_COUNT quality AI articles - $(date +%Y-%m-%d)"
            
            echo "üì§ Pushing $ARTICLE_COUNT new articles..."
            if git push; then
              echo "‚úÖ Successfully pushed $ARTICLE_COUNT new articles"
            else
              echo "‚ùå Push failed, attempting force push with lease..."
              git push --force-with-lease || exit 1
            fi
          else
            echo "‚ÑπÔ∏è No new articles to commit"
          fi