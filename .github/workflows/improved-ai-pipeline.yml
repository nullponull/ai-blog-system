name: AI Article Generation & Publishing Pipeline

on:
  schedule:
    - cron: '15 */8 * * *'  # 8ÊôÇÈñì„Åî„Å®
  workflow_dispatch:

jobs:
  generate-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          npm install -g @google/gemini-cli textlint textlint-rule-preset-japanese textlint-rule-preset-ja-technical-writing @textlint-ja/textlint-rule-preset-ai-writing
          pip install sentence-transformers scikit-learn numpy requests pillow
          mkdir -p _temp assets/images/posts

      - name: Generate topics and articles
        shell: bash
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY2 }}
        run: |
          export GEMINI_API_KEY="$GEMINI_API_KEY"
          
          echo "üéØ Generating AI topics..."
          echo "Debug: GEMINI_API_KEY is $([ -n "$GEMINI_API_KEY" ] && echo "set (length: ${#GEMINI_API_KEY})" || echo "NOT SET")"
          
          # GitHub ActionsÁí∞Â¢É„Åß„ÅÆAPI keyË™çË®ºÁ¢∫Ë™ç
          if [ -z "$GEMINI_API_KEY" ]; then
            echo "‚ùå GEMINI_API_KEY is not set. Please check GitHub Secrets configuration."
            exit 1
          fi
          
          # Gemini CLI with API key authentication
          echo "Testing Gemini API connection..."
          if ! gemini --version; then
            echo "‚ùå Gemini CLI not found or not working"
            exit 1
          fi
          
          # Optimized API call: Generate 8 diverse topics in one call (API optimization)
          echo "üéØ Optimized topic generation (single API call)..."
          OPTIMIZED_PROMPT="WebSearch: AIÊ•≠Áïå ÊúÄÊñ∞„Éã„É•„Éº„Çπ„ÄÇ

**„Éü„ÉÉ„Ç∑„Éß„É≥:** AIÊ•≠ÁïåÂ∞ÇÈñÄË®òËÄÖ„Å®„Åó„Å¶„ÄÅWebÊ§úÁ¥¢„ÅßÊúÄÊñ∞ÊÉÖÂ†±„ÇíË™øÊüª„Åó„ÄÅÂ§öÊßò„Å™ÂàÜÈáé„Åã„Çâ8ÂÄã„ÅÆË≥™„ÅÆÈ´ò„ÅÑË©±È°å„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

**Â§öÊßòÊÄßË¶Å‰ª∂ÔºàÂêÑÂàÜÈáé2ÂÄã„Åö„Å§Ôºâ:**
1. **‰ºÅÊ•≠„Éª„Éì„Ç∏„Éç„ÇπÂãïÂêë:** Êñ∞Ë£ΩÂìÅÁô∫Ë°®„ÄÅÊäïË≥á„ÉªË≥áÈáëË™øÈÅî„ÄÅM&A„ÄÅIPO„ÄÅÊà¶Áï•Áô∫Ë°®
2. **ÊäÄË°ì„ÉªÁ†îÁ©∂ÈñãÁô∫:** Êñ∞ÊäÄË°ì„ÄÅÁ†îÁ©∂ÊàêÊûú„ÄÅË´ñÊñáÁô∫Ë°®„ÄÅ„Ç¢„É´„Ç¥„É™„Ç∫„É†Èù©Êñ∞
3. **ÊîøÁ≠ñ„ÉªË¶èÂà∂:** Ê≥ïË¶èÂà∂„ÄÅÊîøÂ∫úÊîøÁ≠ñ„ÄÅÂõΩÈöõÂçîÂäõ„ÄÅ„Ç¨„Ç§„Éâ„É©„Ç§„É≥Á≠ñÂÆö
4. **Á§æ‰ºöÂÆüË£Ö„Éª‰∫ã‰æã:** ‰ºÅÊ•≠Â∞éÂÖ•‰∫ã‰æã„ÄÅÊ•≠ÁïåÂ§âÈù©„ÄÅÂÆüÁî®Âåñ„ÉªÂïÜÁî®Âåñ‰∫ã‰æã

**Âá∫ÂäõÂΩ¢ÂºèÔºàÂé≥ÂØÜ„Å´Âæì„Å£„Å¶„Åè„Å†„Åï„ÅÑÔºâ:**
1. [‰ºÅÊ•≠„Éª„Éì„Ç∏„Éç„Çπ] ÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°åÔºö‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„ÉªÈáëÈ°çÁ≠â„ÇíÂê´„ÇÄ
2. [‰ºÅÊ•≠„Éª„Éì„Ç∏„Éç„Çπ] ÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°åÔºö‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„ÉªÈáëÈ°çÁ≠â„ÇíÂê´„ÇÄ  
3. [ÊäÄË°ì„ÉªÁ†îÁ©∂] ÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°åÔºöÁ†îÁ©∂ËÄÖÂêç„ÉªÂ§ßÂ≠¶Âêç„ÉªÊäÄË°ìÂêç„ÇíÂê´„ÇÄ
4. [ÊäÄË°ì„ÉªÁ†îÁ©∂] ÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°åÔºöÁ†îÁ©∂ËÄÖÂêç„ÉªÂ§ßÂ≠¶Âêç„ÉªÊäÄË°ìÂêç„ÇíÂê´„ÇÄ
5. [ÊîøÁ≠ñ„ÉªË¶èÂà∂] ÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°åÔºöÂõΩÂêç„ÉªÊ©üÈñ¢Âêç„ÉªÊîøÁ≠ñÂêç„ÇíÂê´„ÇÄ
6. [ÊîøÁ≠ñ„ÉªË¶èÂà∂] ÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°åÔºöÂõΩÂêç„ÉªÊ©üÈñ¢Âêç„ÉªÊîøÁ≠ñÂêç„ÇíÂê´„ÇÄ
7. [Á§æ‰ºöÂÆüË£Ö] ÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°åÔºö‰ºÅÊ•≠Âêç„Éª‰∫ã‰æã„ÉªÂΩ±Èüø„ÇíÂê´„ÇÄ
8. [Á§æ‰ºöÂÆüË£Ö] ÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°åÔºö‰ºÅÊ•≠Âêç„Éª‰∫ã‰æã„ÉªÂΩ±Èüø„ÇíÂê´„ÇÄ

**ÂìÅË≥™Ë¶Å‰ª∂:**
- WebÊ§úÁ¥¢„Å´„Çà„ÇãÊúÄÊñ∞„ÉªÊ≠£Á¢∫„Å™ÊÉÖÂ†±‰ΩøÁî®
- ÂÖ∑‰ΩìÁöÑ„Å™Âõ∫ÊúâÂêçË©ûÔºà‰ºÅÊ•≠Âêç„Éª‰∫∫Âêç„ÉªË£ΩÂìÅÂêç„ÉªÊï∞ÂÄ§ÔºâÂøÖÈ†à
- ÂêÑË©±È°å„ÅØÊôÇ‰∫ãÊÄß„Å®Â∞ÇÈñÄÊÄß„Çí‰∏°Á´ã"
          
          if gemini -m "gemini-2.5-flash" --prompt "$OPTIMIZED_PROMPT" > _temp/topics.txt 2>_temp/gemini_error.log; then
            echo "‚úÖ Optimized topic generation successful (1 API call for 8 topics)"
          else
            echo "‚ùå Optimized call failed, trying fallback..."
            cat _temp/gemini_error.log || echo "No error log available"
            
            FALLBACK_PROMPT="AIÊ•≠Áïå„ÅÆÊúÄÊñ∞„Éà„É¨„É≥„Éâ„Åã„ÇâÂ§öÊßò„Å™8ÂÄã„ÅÆË©±È°å„Çí„É™„Çπ„Éà„Ç¢„ÉÉ„Éó„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

**ÂàÜÈáéÂà•Ë¶Å‰ª∂ÔºàÂêÑ2ÂÄãÔºâ:**
- ‰ºÅÊ•≠„Éª„Éì„Ç∏„Éç„Çπ: Êñ∞Ë£ΩÂìÅ„ÄÅÊäïË≥á„ÄÅM&A„ÄÅÊà¶Áï•Áô∫Ë°®
- ÊäÄË°ì„ÉªÁ†îÁ©∂: Êñ∞ÊäÄË°ì„ÄÅÁ†îÁ©∂ÊàêÊûú„ÄÅË´ñÊñá„ÄÅ„Ç¢„É´„Ç¥„É™„Ç∫„É†
- ÊîøÁ≠ñ„ÉªË¶èÂà∂: Ê≥ïË¶èÂà∂„ÄÅÊîøÂ∫úÊîøÁ≠ñ„ÄÅÂõΩÈöõÂçîÂäõ
- Á§æ‰ºöÂÆüË£Ö: Â∞éÂÖ•‰∫ã‰æã„ÄÅÊ•≠ÁïåÂ§âÈù©„ÄÅÂÆüÁî®Âåñ

**Âá∫Âäõ:** 1-8„ÅÆÁï™Âè∑‰ªò„Åç„É™„Çπ„Éà„Åß„ÄÅ‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„Éª‰∫∫Âêç„ÇíÂê´„ÇÄÂÖ∑‰ΩìÁöÑ„Å™Ë©±È°å"
            
            if gemini -m "gemini-2.5-flash" --prompt "$FALLBACK_PROMPT" > _temp/topics.txt 2>_temp/fallback_error.log; then
              echo "‚úÖ Fallback successful (1 API call)"
            else
              echo "‚ùå Topic generation completely failed"
              exit 1
            fi
          fi
          
          echo "Generated topics:"
          head -15 _temp/topics.txt
          
          # Extract and filter duplicate topics
          ALL_GENERATED_TOPICS=$(grep -E "^[0-9]+\." _temp/topics.txt)
          echo "üìù Initial topics generated:"
          echo "$ALL_GENERATED_TOPICS"
          
          if [ -z "$ALL_GENERATED_TOPICS" ]; then
            echo "‚ùå No topics generated, exiting..."
            exit 1
          fi
          
          # Get existing titles for duplicate check (last 5 days)
          if ls _posts/*.md 1> /dev/null 2>&1; then
            find _posts -name "*.md" -mtime -5 -exec grep -h "^title:" {} \; 2>/dev/null | sed 's/^title: *["]*\|["]*$//g' > _temp/existing_titles.txt
          else
            touch _temp/existing_titles.txt
          fi
          
          echo "üîç Filtering out duplicate topics..."
          TOPICS=""
          TOPIC_COUNT=0
          RETRY_COUNT=0
          MAX_RETRIES=3
          
          # Filter duplicates and get unique topics (reduced to 5 to stay within API limits)
          while IFS= read -r topic_line && [ $TOPIC_COUNT -lt 5 ]; do
            if [ -z "$topic_line" ]; then
              continue
            fi
            
            topic_title=$(printf '%s\n' "$topic_line" | sed 's/^[0-9]*\. *//')
            echo "üîç Checking: $topic_title"
            
            # Simple keyword-based duplicate check
            is_duplicate=false
            if [ -s _temp/existing_titles.txt ]; then
              # Extract key words from topic (first 3 significant words)
              topic_keywords=$(echo "$topic_title" | sed 's/[()ÔºàÔºâ].*//g' | head -c 30)
              while IFS= read -r existing_title; do
                if [ -n "$existing_title" ] && echo "$existing_title" | grep -qi "$(echo "$topic_keywords" | cut -d' ' -f1-2)"; then
                  echo "‚è≠Ô∏è Duplicate detected: $topic_title (similar to existing: $existing_title)"
                  is_duplicate=true
                  break
                fi
              done < _temp/existing_titles.txt
            fi
            
            if [ "$is_duplicate" = false ]; then
              if [ -z "$TOPICS" ]; then
                TOPICS="$topic_line"
              else
                TOPICS="$TOPICS"$'\n'"$topic_line"
              fi
              TOPIC_COUNT=$((TOPIC_COUNT + 1))
              echo "‚úÖ Added unique topic: $topic_title"
            fi
          done <<< "$ALL_GENERATED_TOPICS"
          
          # Simple selection from the 8 generated topics (no additional API calls)
          echo "üìù Selecting best 5 topics from the 8 generated..."
          
          # Final check: if we still don't have enough topics after all retries
          if [ $TOPIC_COUNT -lt 5 ]; then
            echo "‚ö†Ô∏è Warning: Only $TOPIC_COUNT unique topics found after $MAX_RETRIES retries"
            echo "‚ö†Ô∏è This might indicate topic exhaustion or API issues"
            echo "‚ö†Ô∏è Proceeding with available topics to avoid infinite processing"
          elif [ $TOPIC_COUNT -lt 10 ]; then
            echo "‚ÑπÔ∏è Note: Found $TOPIC_COUNT topics (less than target 10)"
            echo "‚ÑπÔ∏è Proceeding with available unique topics"
          fi
          
          # Ensure we have at least some topics to process  
          if [ $TOPIC_COUNT -eq 0 ]; then
            echo "‚ùå No unique topics found after $MAX_RETRIES retries. This indicates:"
            echo "   - All generated topics were duplicates of recent articles"
            echo "   - API issues preventing topic generation"
            echo "   - Possible topic exhaustion in current search domains"
            echo "‚ùå Exiting to prevent infinite processing."
            exit 1
          elif [ $TOPIC_COUNT -lt 3 ]; then
            echo "‚ö†Ô∏è Very few unique topics ($TOPIC_COUNT) found. This may indicate topic saturation."
            echo "‚ö†Ô∏è Consider adjusting search criteria or reducing posting frequency."
          fi
          
          echo "üìä Final unique topics selected: $TOPIC_COUNT/5"
          TOPICS=$(printf '%s\n' "$TOPICS")
          echo "Final topics for processing:"
          echo "$TOPICS"
          
          # Optimized batch article generation (reduce API calls from 5 to 2-3)
          echo "üìù Batch article generation with integrated Mermaid diagrams..."
          
          # Create Python script for batch processing
          python3 -c "
import sys
import os
import re
from datetime import datetime

# Read topics
with open('_temp/topics_for_loop.txt', 'r', encoding='utf-8') as f:
    all_topics = [line.strip() for line in f if line.strip()]

print(f'üìä Processing {len(all_topics)} topics in 2 batch API calls...')

# Split topics into 2 batches for API optimization
batch1 = all_topics[:3] if len(all_topics) >= 3 else all_topics
batch2 = all_topics[3:5] if len(all_topics) > 3 else []

batches = [batch1, batch2] if batch2 else [batch1]

for batch_num, topics_batch in enumerate(batches, 1):
    if not topics_batch:
        continue
        
    print(f'üìù Processing batch {batch_num}: {len(topics_batch)} articles')
    
    # Create batch prompt
    batch_prompt = f'''WebSearch: {datetime.now().strftime('%YÂπ¥%mÊúà%dÊó•')} AIÊ•≠Áïå„ÄÇ

**„Éü„ÉÉ„Ç∑„Éß„É≥:** ‰ª•‰∏ã„ÅÆ{len(topics_batch)}ÂÄã„ÅÆË©±È°å„Å´„Å§„ÅÑ„Å¶„ÄÅ„Åù„Çå„Åû„ÇåÁã¨Á´ã„Åó„ÅüÂ∞ÇÈñÄË®ò‰∫ã„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

**Ë®ò‰∫ãË¶Å‰ª∂:**
- WebÊ§úÁ¥¢„ÅßÊúÄÊñ∞ÊÉÖÂ†±„ÇíË™øÊüª
- ‰ºÅÊ•≠Âêç„ÉªË£ΩÂìÅÂêç„Éª‰∫∫Âêç„ÉªÊï∞ÂÄ§„Éá„Éº„Çø„ÇíÊ≠£Á¢∫„Å´Ë®òËºâ
- ÊäÄË°ìËÄÖ„ÉªÊäïË≥áÂÆ∂Âêë„Åë„ÅÆË©≥Á¥∞ÂàÜÊûê
- 3000-4000ÊñáÂ≠óÁ®ãÂ∫¶
- AI„Å£„ÅΩ„ÅÑË°®Áèæ„ÇíÈÅø„ÅëËá™ÁÑ∂„Å™ÊñáÁ´†

**Âá∫ÂäõÂΩ¢ÂºèÔºàÂé≥ÂØÜ„Å´Âæì„Å£„Å¶„Åè„Å†„Åï„ÅÑÔºâ:**
=== ARTICLE_START_1 ===
# [Ë©±È°å1„ÅÆ„Çø„Ç§„Éà„É´]

## Ê¶ÇË¶Å„Å®ËÉåÊôØ
[Ë©≥Á¥∞„Å™ÂÜÖÂÆπ]

## Ë©≥Á¥∞„Å™ÊäÄË°ì„Éª„Éì„Ç∏„Éç„ÇπÂÜÖÂÆπ
[Ë©≥Á¥∞„Å™ÂÜÖÂÆπ]

## Â∏ÇÂ†¥„ÉªÁ´∂Âêà„Å∏„ÅÆÂΩ±Èüø
[Ë©≥Á¥∞„Å™ÂÜÖÂÆπ]

## Âõ≥Ëß£„Éª„Ç∑„Çπ„ÉÜ„É†ÊßãÊàê
**Mermaid„Éï„É≠„Éº„ÉÅ„É£„Éº„Éà:**
```mermaid
[Ë®ò‰∫ãÂÜÖÂÆπ„Å´ÈÅ©„Åó„ÅüMermaidÂõ≥Ëß£„ÇíÁîüÊàê]
// ‰æãÔºö„Éó„É≠„Çª„Çπ„Éï„É≠„Éº„ÄÅ„Ç∑„Çπ„ÉÜ„É†ÊßãÊàê„ÄÅÈñ¢‰øÇÊÄßÂõ≥„Å™„Å©
// Âà∂Á¥ÑÔºösubgraph„ÄÅlinkStyle„ÄÅfont-weight‰ΩøÁî®‰∏çÂèØ
// Êé®Â•®ÔºöÂü∫Êú¨„Éé„Éº„ÉâÂΩ¢Áä∂[]„ÄÅ{}„ÄÅ()„ÄÅËâ≤ÂàÜ„ÅëstyleÊñá
```

## ‰ªäÂæå„ÅÆÂ±ïÊúõ
[Ë©≥Á¥∞„Å™ÂÜÖÂÆπ]
=== ARTICLE_END_1 ===

=== ARTICLE_START_2 ===
# [Ë©±È°å2„ÅÆ„Çø„Ç§„Éà„É´]
[ÂêåÊßò„ÅÆÊßãÊàê„ÅßMermaidÂõ≥Ëß£„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂê´„ÇÄ]
=== ARTICLE_END_2 ===

**ÂØæË±°Ë©±È°å:**'''
    
    for i, topic_line in enumerate(topics_batch, 1):
        topic = re.sub(r'^[0-9]*\. *', '', topic_line)
        batch_prompt += f'''
{i}. {topic}'''
    
    # Write batch prompt to file for gemini CLI
    with open(f'_temp/batch_prompt_{batch_num}.txt', 'w', encoding='utf-8') as f:
        f.write(batch_prompt)
    
    print(f'‚úÖ Batch {batch_num} prompt prepared')
"
          
          # Execute batch API calls
          BATCH_SUCCESS=0
          for batch_num in 1 2; do
            if [ ! -f "_temp/batch_prompt_${batch_num}.txt" ]; then
              continue
            fi
            
            echo "üöÄ Executing batch $batch_num API call..."
            
            if timeout 180 gemini -m "gemini-2.5-flash" < "_temp/batch_prompt_${batch_num}.txt" > "_temp/batch_response_${batch_num}.md" 2>_temp/batch_error_${batch_num}.log; then
              echo "‚úÖ Batch $batch_num API call successful"
              BATCH_SUCCESS=$((BATCH_SUCCESS + 1))
            else
              echo "‚ùå Batch $batch_num failed"
              cat _temp/batch_error_${batch_num}.log 2>/dev/null || echo "No error log"
              
              if grep -q "429\|RESOURCE_EXHAUSTED\|quota" _temp/batch_error_${batch_num}.log 2>/dev/null; then
                echo "üö´ Rate limit reached, stopping batch processing"
                break
              fi
            fi
          done
          
          # Process batch responses with integrated Mermaid processing
          python3 -c "
import sys
import os
import re
import base64
import requests
from datetime import datetime
from PIL import Image
import io

generated_count = 0
current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S %z')

def mermaid_to_image_simple(mermaid_code, output_path):
    '''Convert Mermaid to image with simplified syntax'''
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Simplify Mermaid code to avoid 404 errors
        lines = mermaid_code.split('\n')
        simplified_lines = []
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('//'):
                continue
            # Remove complex styles that cause 404s    
            if 'subgraph' in line.lower() or 'linkStyle' in line or 'font-weight' in line:
                continue
            simplified_lines.append(line)
        
        simplified_code = '\n'.join(simplified_lines)
        
        # Convert to image via Mermaid.ink
        encoded_mermaid = base64.b64encode(simplified_code.encode('utf-8')).decode('utf-8')
        mermaid_url = f'https://mermaid.ink/img/{encoded_mermaid}'
        
        if len(mermaid_url) > 1800:
            print(f'‚ö†Ô∏è Mermaid URL too long, skipping: {os.path.basename(output_path)}')
            return False
            
        response = requests.get(mermaid_url, timeout=30)
        if response.status_code == 200:
            image = Image.open(io.BytesIO(response.content))
            
            # Convert to JPEG with white background
            if image.mode in ('RGBA', 'LA', 'P'):
                background = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'P':
                    image = image.convert('RGBA')
                background.paste(image, mask=image.split()[-1] if image.mode == 'RGBA' else None)
                image = background
            
            image.save(output_path, 'JPEG', quality=85, optimize=True)
            print(f'‚úÖ Mermaid image saved: {os.path.basename(output_path)}')
            return True
        else:
            print(f'‚ùå Mermaid.ink failed ({response.status_code}): {os.path.basename(output_path)}')
            return False
    except Exception as e:
        print(f'‚ùå Mermaid processing error: {e}')
        return False

for batch_num in [1, 2]:
    response_file = f'_temp/batch_response_{batch_num}.md'
    if not os.path.exists(response_file):
        continue
        
    try:
        with open(response_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Extract individual articles using markers
        articles = re.findall(r'=== ARTICLE_START_(\d+) ===(.*?)=== ARTICLE_END_\1 ===', content, re.DOTALL)
        
        for article_idx, article_content in articles:
            article_content = article_content.strip()
            if not article_content:
                continue
                
            # Extract title
            title_match = re.search(r'^# (.+)$', article_content, re.MULTILINE)
            if not title_match:
                continue
                
            title = title_match.group(1).strip()
            generated_count += 1
            
            # Create filename
            slug = re.sub(r'[^a-zA-Z0-9\s]', '', title.lower())[:50]
            slug = re.sub(r'\s+', '-', slug).strip('-')
            filename = f'{datetime.now().strftime(\"%Y-%m-%d\")}-{generated_count}-{slug}.md'
            
            # Extract and process Mermaid diagrams
            mermaid_matches = re.findall(r'```mermaid\s*(.*?)\s*```', article_content, re.DOTALL)
            for i, mermaid_code in enumerate(mermaid_matches, 1):
                mermaid_code = mermaid_code.strip()
                if mermaid_code and not mermaid_code.startswith('[Ë®ò‰∫ãÂÜÖÂÆπ„Å´ÈÅ©„Åó„Åü'):
                    # Generate Mermaid image
                    mermaid_filename = f'{datetime.now().strftime(\"%Y-%m-%d\")}-{generated_count}-mermaid-{i}.jpg'
                    mermaid_path = f'assets/images/posts/{mermaid_filename}'
                    
                    if mermaid_to_image_simple(mermaid_code, mermaid_path):
                        # Replace Mermaid code block with image reference
                        image_md = f'![{title} - Âõ≥Ëß£ {i}](/assets/images/posts/{mermaid_filename})'
                        article_content = article_content.replace(f'```mermaid\n{mermaid_code}\n```', image_md)
            
            # Determine category
            category = 'ÊúÄÊñ∞ÂãïÂêë'
            if any(word in title for word in ['Á†îÁ©∂', 'Ë´ñÊñá', 'Â≠¶‰ºö']):
                category = 'Á†îÁ©∂Ë´ñÊñá'
            elif any(word in title for word in ['ÊäÄË°ì', '„Ç¢„É´„Ç¥„É™„Ç∫„É†', '‰ªïÁµÑ„Åø']):
                category = 'ÊäÄË°ìËß£Ë™¨'
            elif any(word in title for word in ['Â∞éÂÖ•', '‰∫ã‰æã', 'Ê¥ªÁî®']):
                category = 'ÂÆüË£Ö‰∫ã‰æã'
            elif any(word in title for word in ['Â∏ÇÂ†¥', 'ÂàÜÊûê', '‰∫àÊ∏¨']):
                category = 'Ê•≠ÁïåÂàÜÊûê'
            
            # Create frontmatter
            frontmatter = f'''---
layout: post
title: \"{title}\"
date: {current_date}
categories: [\"{category}\"]
tags: [\"AI\", \"ÊúÄÊñ∞„Éã„É•„Éº„Çπ\", \"ÊäÄË°ìÂãïÂêë\"]
author: \"AIË®ò‰∫ãÁîüÊàê„Ç∑„Çπ„ÉÜ„É†\"
excerpt: \"AIÊ•≠Áïå„ÅÆÊúÄÊñ∞ÂãïÂêë„Å´„Å§„ÅÑ„Å¶Ë©≥„Åó„ÅèËß£Ë™¨„Åó„Åæ„Åô„ÄÇ\"
reading_time: 8
---

'''
            
            # Write final article file
            with open(f'_temp/temp-{filename}', 'w', encoding='utf-8', errors='ignore') as f:
                f.write(frontmatter + article_content)
            
            print(f'‚úÖ Generated article {generated_count}: {filename}')
    
    except Exception as e:
        print(f'‚ùå Error processing batch {batch_num}: {e}')

print(f'üìä Generated {generated_count} articles with integrated Mermaid diagrams')
"
          
          echo "üìä Batch article generation completed"

      - name: Semantic article selection
        run: |
          echo "üß† Starting semantic analysis..."
          
          # Get existing titles
          if ls _posts/*.md 1> /dev/null 2>&1; then
            find _posts -name "*.md" -mtime -2 -exec grep -h "^title:" {} \; 2>/dev/null | head -15 > _temp/existing_titles.txt
          else
            touch _temp/existing_titles.txt
          fi
          
          # Create Python script for semantic analysis using echo statements
          echo "import os" > _temp/semantic_selector.py
          echo "import glob" >> _temp/semantic_selector.py
          echo "from sentence_transformers import SentenceTransformer" >> _temp/semantic_selector.py
          echo "from sklearn.metrics.pairwise import cosine_similarity" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "print('Loading multilingual sentence transformer model...')" >> _temp/semantic_selector.py
          echo "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "def get_embedding(text):" >> _temp/semantic_selector.py
          echo "    return model.encode([text])" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "def extract_title(filepath):" >> _temp/semantic_selector.py
          echo "    try:" >> _temp/semantic_selector.py
          echo "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:" >> _temp/semantic_selector.py
          echo "            for line in f:" >> _temp/semantic_selector.py
          echo "                if line.startswith('title:'):" >> _temp/semantic_selector.py
          echo "                    return line.replace('title:', '').strip().strip('\"')" >> _temp/semantic_selector.py
          echo "    except Exception as e:" >> _temp/semantic_selector.py
          echo "        print(f'Error reading {filepath}: {e}')" >> _temp/semantic_selector.py
          echo "    return ''" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "existing_titles = []" >> _temp/semantic_selector.py
          echo "if os.path.exists('_temp/existing_titles.txt'):" >> _temp/semantic_selector.py
          echo "    with open('_temp/existing_titles.txt', 'r') as f:" >> _temp/semantic_selector.py
          echo "        existing_titles = [line.strip().replace('title:', '').strip().strip('\"') for line in f.readlines() if line.strip()]" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "temp_articles = glob.glob('_temp/temp-*.md')" >> _temp/semantic_selector.py
          echo "published_count = 0" >> _temp/semantic_selector.py
          echo "print(f'Evaluating {len(temp_articles)} articles...')" >> _temp/semantic_selector.py
          echo "" >> _temp/semantic_selector.py
          echo "for filepath in temp_articles:" >> _temp/semantic_selector.py
          echo "    title = extract_title(filepath)" >> _temp/semantic_selector.py
          echo "    if not title:" >> _temp/semantic_selector.py
          echo "        continue" >> _temp/semantic_selector.py
          echo "    max_similarity = 0.0" >> _temp/semantic_selector.py
          echo "    if existing_titles:" >> _temp/semantic_selector.py
          echo "        title_embedding = get_embedding(title)" >> _temp/semantic_selector.py
          echo "        for existing_title in existing_titles:" >> _temp/semantic_selector.py
          echo "            if existing_title:" >> _temp/semantic_selector.py
          echo "                existing_embedding = get_embedding(existing_title)" >> _temp/semantic_selector.py
          echo "                similarity = cosine_similarity(title_embedding.reshape(1, -1), existing_embedding.reshape(1, -1))[0][0]" >> _temp/semantic_selector.py
          echo "                max_similarity = max(max_similarity, similarity)" >> _temp/semantic_selector.py
          echo "    is_duplicate = max_similarity > 0.75" >> _temp/semantic_selector.py
          echo "    if not is_duplicate:" >> _temp/semantic_selector.py
          echo "        final_name = os.path.basename(filepath).replace('temp-', '')" >> _temp/semantic_selector.py
          echo "        os.rename(filepath, f'_posts/{final_name}')" >> _temp/semantic_selector.py
          echo "        print(f'Published: {final_name}')" >> _temp/semantic_selector.py
          echo "        published_count += 1" >> _temp/semantic_selector.py
          echo "    else:" >> _temp/semantic_selector.py
          echo "        print(f'Skipped duplicate: {title[:50]}... (similarity: {max_similarity:.3f})')" >> _temp/semantic_selector.py
          echo "print(f'Published {published_count} unique articles')" >> _temp/semantic_selector.py
          
          # Run the semantic selector
          python3 _temp/semantic_selector.py

      - name: Improve article quality with textlint
        run: |
          echo "üìù Improving article quality with textlint..."
          
          # Run textlint on all generated articles with AI writing detection
          for article in _posts/*.md; do
            if [ -f "$article" ]; then
              echo "üîç Checking: $(basename "$article")"
              
              # Run textlint with detailed output
              if ! textlint "$article"; then
                echo "‚ö†Ô∏è TextLint issues found in $(basename "$article")"
              fi
              
              # Fix common AI-generated patterns and issues
              sed -i 's/„ÄÅ„ÄÅ/„ÄÅ/g' "$article"
              sed -i 's/„ÄÇ„ÄÇ/„ÄÇ/g' "$article"
              # Remove common AI metadata patterns
              sed -i '/^AI „Å´„Çà„Å£„Å¶ÁîüÊàê/d' "$article"
              sed -i '/^„Åì„ÅÆË®ò‰∫ã„ÅØ AI „ÅßÁîüÊàê/d' "$article"
              sed -i '/^‚Äª „Åì„ÅÆË®ò‰∫ã„ÅØ AI/d' "$article"
              # Remove excessive emphasis
              sed -i 's/\*\*\([^*]*\)\*\*/\1/g' "$article" | head -c 0
              # Clean up list formatting
              sed -i 's/^- \*\*/- /g' "$article"
              
              echo "‚úÖ Improved: $(basename "$article")"
            fi
          done

      - name: Generate images for articles
        env:
          HUGGINGFACE_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY2 }}
        run: |
          export HUGGINGFACE_TOKEN="$HUGGINGFACE_TOKEN"
          export GEMINI_API_KEY="$GEMINI_API_KEY"
          
          echo "üé® Generating images for published articles..."
          # Install required dependencies
          pip install requests pillow
          
          # 1. Generate featured images for articles  
          python3 scripts/image_generator.py
          
          # 2. Mermaid diagrams now integrated into article generation (0 additional API calls)
          echo "‚úÖ Mermaid diagrams integrated into batch article generation (0 additional API calls)"

      - name: Setup Ruby and Jekyll
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.1'
          bundler-cache: true

      - name: Build and Deploy
        run: |
          bundle install
          bundle exec jekyll build

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './_site'

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4

      - name: Commit generated articles
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          if [ -n "$(git status --porcelain _posts/)" ]; then
            ARTICLE_COUNT=$(ls _posts/$(date +%Y-%m-%d)-*.md 2>/dev/null | wc -l)
            git add _posts/
            git commit -m "ü§ñ Add $ARTICLE_COUNT unique AI articles - $(date +%Y-%m-%d)"
            git push
            echo "‚úÖ Committed $ARTICLE_COUNT new articles"
          else
            echo "‚ÑπÔ∏è  No new articles to commit"
          fi