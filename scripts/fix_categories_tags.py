#!/usr/bin/env python3
"""
Fix categories and tags in blog posts based on content analysis
"""

import os
import glob
import re
from pathlib import Path

def analyze_content_for_categories(filepath):
    """Analyze article content to determine appropriate categories"""
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read().lower()
            title_content = content[:1000]  # Focus on title and first part
            
        # Define category keywords based on _config.yml definitions
        categories = {
            "AIæœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹": ["æœ€æ–°", "ç™ºè¡¨", "ãƒªãƒªãƒ¼ã‚¹", "æ–°è£½å“", "ãƒ‹ãƒ¥ãƒ¼ã‚¹", "ç™ºè¡¨ä¼š", "ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ", "æ–°æ©Ÿèƒ½", "é€Ÿå ±"],
            "AIå°å…¥æˆ¦ç•¥": ["æŠ•è³‡", "å¸‚å ´", "æ ªä¾¡", "è³‡é‡‘èª¿é”", "è©•ä¾¡é¡", "roi", "ãƒªã‚¿ãƒ¼ãƒ³", "æŠ•è³‡å®¶", "è³‡æœ¬", "ãƒ•ã‚¡ãƒ³ãƒ‰", "å„„ãƒ‰ãƒ«", "å…†ãƒ‰ãƒ«", "æˆ¦ç•¥", "ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—"],
            "AIæŠ€è¡“ã‚¬ã‚¤ãƒ‰": ["æŠ€è¡“", "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£", "api", "sdk", "ãƒ—ãƒ­ã‚°ãƒ©ãƒ ", "ã‚³ãƒ¼ãƒ‰", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "ãƒ¢ãƒ‡ãƒ«", "å­¦ç¿’", "rag", "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"],
            "å°å…¥äº‹ä¾‹": ["å®Ÿè£…", "å°å…¥", "æ´»ç”¨", "äº‹ä¾‹", "ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£", "å±•é–‹", "é©ç”¨", "é‹ç”¨", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ"],
            "ç ”ç©¶è«–æ–‡": ["ç ”ç©¶", "è«–æ–‡", "å­¦è¡“", "å®Ÿé¨“", "ãƒ‡ãƒ¼ã‚¿", "çµæœ", "æ¤œè¨¼", "åˆ†æ"],
            "æ¥­ç•Œåˆ¥AIæ´»ç”¨": ["è£½é€ ", "é‡‘è", "åŒ»ç™‚", "å°å£²", "ç‰©æµ", "æ•™è‚²", "å»ºè¨­", "ä¸å‹•ç”£", "ä¿é™º", "è¾²æ¥­"]
        }
        
        # Count keyword matches for each category
        category_scores = {}
        for category, keywords in categories.items():
            score = sum(title_content.count(keyword) for keyword in keywords)
            if score > 0:
                category_scores[category] = score
        
        # Return the category with highest score, default to "æœ€æ–°å‹•å‘"
        if category_scores:
            return max(category_scores, key=category_scores.get)
        else:
            return "æœ€æ–°å‹•å‘"
            
    except Exception as e:
        print(f'Error analyzing {filepath}: {e}')
        return "æœ€æ–°å‹•å‘"

def analyze_content_for_tags(filepath):
    """Analyze article content to determine appropriate tags"""
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read().lower()
            
        # Define tag keywords
        tag_keywords = {
            "OpenAI": ["openai", "gpt", "chatgpt", "dall-e", "sam altman"],
            "Google": ["google", "gemini", "bard", "deepmind", "tpu", "alphabet"],
            "Microsoft": ["microsoft", "azure", "copilot", "satya nadella"],
            "Meta": ["meta", "facebook", "llama", "mark zuckerberg"],
            "NVIDIA": ["nvidia", "gpu", "cuda", "blackwell", "hopper", "jensen huang"],
            "Amazon": ["amazon", "aws", "alexa", "anthropic", "claude"],
            "Apple": ["apple", "siri", "ios", "tim cook"],
            "Anthropic": ["anthropic", "claude", "constitutional ai"],
            "æŠ•è³‡": ["æŠ•è³‡", "è³‡é‡‘èª¿é”", "è©•ä¾¡é¡", "æ ªä¾¡", "å¸‚å ´", "ipo"],
            "æŠ€è¡“é©æ–°": ["é©æ–°", "breakthrough", "innovation", "ç”»æœŸçš„"],
            "ãƒãƒƒãƒ—": ["ãƒãƒƒãƒ—", "chip", "gpu", "tpu", "semiconductor", "åŠå°ä½“"],
            "ã‚¯ãƒ©ã‚¦ãƒ‰": ["ã‚¯ãƒ©ã‚¦ãƒ‰", "cloud", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼", "aws", "azure", "gcp"],
            "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ": ["ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ", "agent", "è‡ªå‹•åŒ–", "automation"],
            "ãƒ‡ãƒ¼ã‚¿": ["ãƒ‡ãƒ¼ã‚¿", "data", "ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿", "dataset"],
            "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£": ["ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "security", "ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼", "privacy"],
            "å€«ç†": ["å€«ç†", "ethics", "è²¬ä»»", "å®‰å…¨æ€§", "bias"],
            "åŒ»ç™‚": ["åŒ»ç™‚", "å¥åº·", "ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢", "è¨ºæ–­", "æ²»ç™‚"],
            "è‡ªå‹•è»Š": ["è‡ªå‹•è»Š", "è»Šè¼‰", "è‡ªå‹•é‹è»¢", "tesla"],
            "ç”ŸæˆAI": ["ç”Ÿæˆai", "generative", "ç”»åƒç”Ÿæˆ", "ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"],
            "LLM": ["llm", "large language model", "è¨€èªãƒ¢ãƒ‡ãƒ«", "transformer"]
        }
        
        # Find matching tags
        matched_tags = ["AI", "æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹", "æŠ€è¡“å‹•å‘"]  # Default tags
        
        for tag, keywords in tag_keywords.items():
            if any(keyword in content for keyword in keywords):
                if tag not in matched_tags:
                    matched_tags.append(tag)
        
        # Limit to 6 tags maximum for better organization
        return matched_tags[:6]
            
    except Exception as e:
        print(f'Error analyzing tags for {filepath}: {e}')
        return ["AI", "æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹", "æŠ€è¡“å‹•å‘"]

def update_frontmatter(filepath):
    """Update the frontmatter of a markdown file"""
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # Check if file is empty or corrupted
        if not content.strip():
            print(f'âš ï¸ Empty file: {filepath}')
            return False
        
        lines = content.split('\n')
        
        # Find frontmatter boundaries
        frontmatter_start = -1
        frontmatter_end = -1
        
        for i, line in enumerate(lines):
            if line.strip() == '---':
                if frontmatter_start == -1:
                    frontmatter_start = i
                else:
                    frontmatter_end = i
                    break
        
        if frontmatter_start == -1 or frontmatter_end == -1:
            print(f'âš ï¸ No valid frontmatter found in {filepath}')
            return False
        
        # Analyze content for better categories and tags
        new_category = analyze_content_for_categories(filepath)
        new_tags = analyze_content_for_tags(filepath)
        
        # Update frontmatter lines
        updated_lines = []
        category_updated = False
        tags_updated = False
        
        for i, line in enumerate(lines):
            if frontmatter_start < i < frontmatter_end:
                if line.startswith('categories:'):
                    updated_lines.append(f'categories: ["{new_category}"]')
                    category_updated = True
                elif line.startswith('tags:'):
                    tag_str = ', '.join(f'"{tag}"' for tag in new_tags)
                    updated_lines.append(f'tags: [{tag_str}]')
                    tags_updated = True
                else:
                    updated_lines.append(line)
            else:
                updated_lines.append(line)
        
        # Write updated content back
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(updated_lines))
        
        return True
        
    except Exception as e:
        print(f'âŒ Failed to update {filepath}: {e}')
        return False

def main():
    """Fix categories and tags in all blog posts"""
    print('ğŸ”§ Fixing categories and tags in blog posts...')
    
    # Find all markdown files in _posts
    post_files = glob.glob('_posts/*.md')
    fixed_count = 0
    
    for filepath in post_files:
        filename = os.path.basename(filepath)
        
        if update_frontmatter(filepath):
            print(f'âœ… Updated: {filename}')
            fixed_count += 1
        else:
            print(f'â­ï¸  Skipped: {filename}')
    
    print(f'\nğŸ“Š Updated {fixed_count} files with better categories and tags')

if __name__ == '__main__':
    main()