#!/usr/bin/env python3
"""
Semantic article selector using sentence transformers
Removes duplicate articles based on semantic similarity
"""

import os
import glob
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

def get_embedding(model, text):
    """Get text embedding using sentence transformer"""
    return model.encode([text])

def extract_title(filepath):
    """Extract title from markdown file"""
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if line.startswith('title:'):
                    return line.replace('title:', '').strip().strip('"')
    except Exception as e:
        print(f'Error reading {filepath}: {e}')
    return ''

def extract_date(filepath):
    """Extract date from markdown file"""
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                if line.startswith('date:'):
                    date_str = line.replace('date:', '').strip()
                    return date_str.split()[0]  # Get YYYY-MM-DD part
    except Exception as e:
        print(f'Error reading date from {filepath}: {e}')
    return ''

def slugify(title):
    """Convert title to URL-friendly slug with improved Japanese handling"""
    import re
    import unicodedata
    
    # Clean the title first
    title = title.strip().replace('"', '')
    
    # Comprehensive Japanese-to-English mapping for better SEO
    japanese_mappings = {
        r'Googleã¨Meta.*è¦‡æ¨©äº‰ã„': 'google-meta-ai-competition',
        r'GPT-5.*è¡æ’ƒ': 'gpt-5-impact',
        r'NVIDIA.*Blackwell': 'nvidia-blackwell',
        r'OpenAI.*è‡ªç¤¾.*ãƒãƒƒãƒ—': 'openai-custom-chip',
        r'Microsoft.*AI.*æŠ•è³‡': 'microsoft-ai-investment',
        r'ã‚¯ãƒ©ã‚¦ãƒ‰.*è¦‡æ¨©': 'cloud-ai-competition',
        r'é‡åŠ›æ³¢æ¤œå‡º.*AI': 'gravitational-wave-ai',
        r'Broadcom.*OpenAI': 'broadcom-openai-deal',
        
        # General terms
        r'äººå·¥çŸ¥èƒ½|AIæŠ€è¡“|æ±ç”¨äººå·¥çŸ¥èƒ½|AGI': 'ai',
        r'æŠ•è³‡.*åˆ†æ|å¸‚å ´.*åˆ†æ': 'investment-analysis',
        r'æŠ€è¡“.*è§£èª¬|æŠ€è¡“.*é©æ–°': 'technology-analysis', 
        r'æœ€æ–°.*å‹•å‘|æœ€æ–°.*ãƒ‹ãƒ¥ãƒ¼ã‚¹': 'latest-trends',
        r'å®Ÿè£….*äº‹ä¾‹': 'implementation-case',
        r'ç ”ç©¶.*è«–æ–‡': 'research-paper',
        r'è¦‡æ¨©.*äº‰ã„|ç«¶äº‰.*æ¿€åŒ–': 'market-competition',
        r'æ–°æ™‚ä»£.*æ‹“ã': 'new-era',
        r'æœªæ¥.*åŠ é€Ÿ': 'future-acceleration',
        
        # Company and product names (keep in romaji)
        r'ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆ': 'microsoft',
        r'ã‚°ãƒ¼ã‚°ãƒ«': 'google',
        r'ã‚¢ãƒã‚¾ãƒ³': 'amazon',
        r'ãƒ¡ã‚¿': 'meta',
        r'ã‚¨ãƒŒãƒ“ãƒ‡ã‚£ã‚¢': 'nvidia',
        r'ã‚¢ãƒƒãƒ—ãƒ«': 'apple',
        r'ã‚ªãƒ¼ãƒ—ãƒ³AI': 'openai',
        r'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚³ãƒ ': 'broadcom',
        r'æ—¥ç«‹Vantara': 'hitachi-vantara',
        r'ã‚¨ãƒƒã‚¸AI': 'edge-ai',
    }
    
    # Apply mappings
    slug = title
    for pattern, replacement in japanese_mappings.items():
        slug = re.sub(pattern, replacement, slug, flags=re.IGNORECASE)
    
    # Remove remaining Japanese characters and special characters
    slug = re.sub(r'[^\w\s-]', '', slug)
    slug = re.sub(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+', '', slug)  # Remove Japanese chars
    
    # Clean up and format
    slug = re.sub(r'[-\s]+', '-', slug).strip('-').lower()
    
    # Ensure minimum quality
    if not slug or len(slug.replace('-', '')) < 3:
        return 'ai-article-update'
    
    return slug[:60]  # Increase limit to 60 characters for better descriptiveness

def generate_semantic_filename(filepath):
    """Generate semantic filename based on article title and date"""
    title = extract_title(filepath)
    date = extract_date(filepath)
    
    if not date:
        from datetime import datetime
        date = datetime.now().strftime('%Y-%m-%d')
    
    if not title:
        title = 'AI Article'
    
    slug = slugify(title)
    # Extract topic number from original temp filename for uniqueness
    import re
    match = re.search(r'-(\d+)-\d+\.md$', filepath)
    topic_num = match.group(1) if match else '1'
    
    return f'{date}-{topic_num}-{slug}.md'

def main():
    """Main semantic selector function"""
    print('ğŸ§  Loading multilingual sentence transformer model...')
    try:
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    except Exception as e:
        print(f'âŒ Failed to load semantic model: {e}')
        print('âš ï¸ Falling back to simple keyword-based duplicate detection')
        simple_fallback()
        return
    
    # Load existing titles
    existing_titles = []
    if os.path.exists('_temp/existing_titles.txt'):
        with open('_temp/existing_titles.txt', 'r', encoding='utf-8', errors='ignore') as f:
            existing_titles = [line.strip().replace('title:', '').strip().strip('"') 
                             for line in f.readlines() if line.strip()]
    
    # Process temporary articles
    temp_articles = glob.glob('_temp/temp-*.md')
    published_count = 0
    print(f'ğŸ“ Evaluating {len(temp_articles)} articles using semantic analysis...')
    
    # Create directory if it doesn't exist
    os.makedirs('_posts', exist_ok=True)
    
    for filepath in temp_articles:
        title = extract_title(filepath)
        if not title:
            print(f'âš ï¸ No title found in {filepath}')
            continue
            
        max_similarity = 0.0
        best_match = ""
        
        if existing_titles:
            title_embedding = get_embedding(model, title)
            for existing_title in existing_titles:
                if existing_title:
                    existing_embedding = get_embedding(model, existing_title)
                    similarity = cosine_similarity(
                        title_embedding.reshape(1, -1), 
                        existing_embedding.reshape(1, -1)
                    )[0][0]
                    if similarity > max_similarity:
                        max_similarity = similarity
                        best_match = existing_title
        
        # Multiple duplicate detection strategies
        similarity_duplicate = max_similarity > 0.75
        
        # Check for same-day topic duplicates (GPT-5, NVIDIA, etc.)
        date_today = extract_date(filepath)
        topic_duplicate = False
        
        if date_today:
            # Extract key topic from title
            title_lower = title.lower()
            key_topics = ['gpt-5', 'nvidia', 'google ai', 'microsoft ai', 'openai', 'meta ai']
            
            for topic in key_topics:
                if topic in title_lower:
                    # Check if we already have this topic today
                    today_files = glob.glob(f'_posts/{date_today}*{topic.replace(" ", "*")}*.md')
                    if today_files:
                        topic_duplicate = True
                        print(f'âš ï¸ Same-day topic duplicate detected: {topic} on {date_today}')
                        break
        
        is_duplicate = similarity_duplicate or topic_duplicate
        
        if not is_duplicate:
            final_name = generate_semantic_filename(filepath)
            try:
                os.rename(filepath, f'_posts/{final_name}')
                print(f'âœ… Published: {final_name}')
                published_count += 1
            except Exception as e:
                print(f'âŒ Failed to publish {final_name}: {e}')
        else:
            print(f'â­ï¸ Skipped duplicate: {title[:50]}...')
            print(f'   Similarity: {max_similarity:.3f} with "{best_match[:30]}..."')
            # Clean up duplicate file
            try:
                os.remove(filepath)
            except:
                pass
    
    print(f'ğŸ“Š Published {published_count} unique articles via semantic analysis')

def simple_fallback():
    """Fallback to simple keyword-based duplicate detection"""
    print('ğŸ”§ Using simple keyword-based duplicate detection...')
    
    # Load existing titles for keyword matching
    existing_keywords = set()
    if os.path.exists('_temp/existing_titles.txt'):
        with open('_temp/existing_titles.txt', 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                title = line.strip().replace('title:', '').strip().strip('"')
                if title:
                    # Extract first few keywords
                    words = title.split()[:3]
                    existing_keywords.update(word.lower() for word in words if len(word) > 2)
    
    temp_articles = glob.glob('_temp/temp-*.md')
    published_count = 0
    os.makedirs('_posts', exist_ok=True)
    
    for filepath in temp_articles:
        title = extract_title(filepath)
        if not title:
            continue
            
        # Simple keyword-based duplicate check
        title_words = set(word.lower() for word in title.split()[:3] if len(word) > 2)
        overlap = len(title_words.intersection(existing_keywords))
        
        is_duplicate = overlap >= 3  # If 3+ keywords match existing titles (more lenient)
        
        if not is_duplicate:
            final_name = generate_semantic_filename(filepath)
            try:
                os.rename(filepath, f'_posts/{final_name}')
                print(f'âœ… Published: {final_name}')
                published_count += 1
            except Exception as e:
                print(f'âŒ Failed to publish {final_name}: {e}')
        else:
            print(f'â­ï¸ Skipped potential duplicate: {title[:50]}...')
            try:
                os.remove(filepath)
            except:
                pass
    
    print(f'ğŸ“Š Published {published_count} articles via keyword analysis')

if __name__ == '__main__':
    main()